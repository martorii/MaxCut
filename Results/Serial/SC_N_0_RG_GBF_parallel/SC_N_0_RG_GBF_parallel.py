# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SV7QvGveFSZsQOb-NOlTDRg5hldjesPY
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import os
import time
from scipy import stats
import multiprocessing as mp
import pandas as pd
import datetime

#%matplotlib notebook
# %matplotlib inline

# Max-cut functions
def InitializeGraph(n, min_weight, max_weight, dropout, seed = -1, graph_type="complete", intercalation=None):
    # Randomness
    if graph_type == "complete":
        if seed == -1:
            W = np.random.uniform(low=min_weight, high=max_weight, size=(n,n))
        else:
            local_state = np.random.RandomState(seed)
            W = local_state.uniform(low=min_weight, high=max_weight, size=(n,n))
        # Dropout 
        if dropout>0:
            if seed == -1:
                D = np.random.choice([0,1], size=(n,n), replace=True, p=[dropout, 1-dropout])
            else:
                D = local_state.choice([0,1], size=(n,n), replace=True, p=[dropout, 1-dropout])
            W = np.multiply(W,D)
    elif graph_type == "cycle":
        if seed == -1:
            weights = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            weights = local_state.uniform(low=min_weight, high=max_weight, size=(n,n))
        W = np.zeros((n,n))
        i = 0
        j = 1
        for k in range(n-1):
            W[i,j] = weights[k]
            i+=1
            j+=1
        # Add last weight
        W[0, n-1] = weights[n-1]                
    elif graph_type == "regular_log(n)":
        degseq = np.repeat(np.floor(np.log(n)), n)
        adjMatr = AdjacencyMatrixDegree(degseq)
        if seed == -1:
            W = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            W = local_state.uniform(low=min_weight, high=max_weight, size=n)
        W = np.multiply(W,adjMatr)
    elif graph_type == "intercalate":
        if seed == -1:
            weights = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            weights = local_state.uniform(low=min_weight, high=max_weight, size=(n,n))
        adjMatr = np.zeros((n,n))
        # Fill diagonals above the main diagonal
        for k in range(intercalation):
            for i in range(n-k-1):
                adjMatr[i, i+k+1] = 1
        # Fill upper right corner
        for j in range(intercalation):
            for i in range(j+1):
                adjMatr[j-i, n-i-1]=1
        W = np.multiply(weights, adjMatr)
    # Copy upper diagonal to lower diagonal (only the upper perturbation counts)
    i_lower = np.tril_indices(n, -1)
    W[i_lower] = W.T[i_lower]
    # Make sure diagonal is empty
    np.fill_diagonal(W, 0)
    
    return W

def InitializeFlatGraph(n, min_weight, max_weight, dropout = 0, seed = -1, graph_type = "complete", intercalation = None):
    if graph_type == "complete":
        if seed == -1:
            W = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            W = local_state.uniform(low=min_weight, high=max_weight, size=n)
        if dropout>0:
            if seed == -1:
                D = np.random.choice([0,1], size=n, replace=True, p=[dropout, 1-dropout])
            else:
                D = local_state.choice([0,1], size=n, replace=True, p=[dropout, 1-dropout])
            W = np.multiply(W,D)
    elif graph_type == "cycle":
        if seed == -1:
            weights = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            weights = local_state.uniform(low=min_weight, high=max_weight, size=n)
        W = np.zeros((n,n))
        i = 0
        j = 1
        for k in range(n-1):
            W[i,j] = weights[k]
            i+=1
            j+=1
        # Add last weight
        W[0, n-1] = weights[n-1]
        i_lower = np.tril_indices(n, -1)
        W[i_lower] = W.T[i_lower]
        # Make sure diagonal is empty
        np.fill_diagonal(W, 0)
        W = np.copy(W[np.triu_indices_from(W, k = 1)])
    elif graph_type == "regular_log(n)":
        degseq = np.repeat(np.floor(np.log(n)), n)
        adjMatr = AdjacencyMatrixDegree(degseq)
        if seed == -1:
            W = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            W = local_state.uniform(low=min_weight, high=max_weight, size=n)
        W = np.multiply(W,adjMatr)
        W = np.copy(W[np.triu_indices_from(W, k = 1)])
    elif graph_type == "intercalate":
        W = InitializeGraph(n, min_weight, max_weight, dropout, seed, graph_type, intercalation)
        W = np.copy(W[np.triu_indices_from(W, k = 1)])
    return W

def AdjacencyMatrixDegree(degseq):
    # n is number of vertices  
    n = len(degseq)
    mat = [[0] * n for i in range(n)] 
    for i in range(n): 
        for j in range(i + 1, n): 
            # For each pair of vertex decrement  
            # the degree of both vertex.  
            if (degseq[i] > 0 and degseq[j] > 0): 
                degseq[i] -= 1
                degseq[j] -= 1
                mat[i][j] = 1
                mat[j][i] = 1
    return mat

def GetKRandomPartition(n, k, seed = -1):
    partition = {}
    if seed == -1:
        for i in range(n):
            partition[i]=np.random.randint(low=0, high=k, size=1)[0]
    else:
        local_state = np.random.RandomState(seed)
        for i in range(n):
            partition[i]=local_state.randint(low=0, high=k, size=1)[0]
    return partition

# G stands for special ill-posed matrixes
def GetGWeightMatrix(g):
    if g == 1:
        G = np.array([[0, 7, 0, 0, 0, 0, 0, 0],
             [0, 0, 6, 0, 0, 0, 0, 0],
             [0, 0, 0, 5, 0, 0, 0, 0],
             [0, 0, 0, 0, 1, 3, 0, 0],
             [0, 0, 0, 0, 0, 0, 1, 1],
             [0, 0, 0, 0, 0, 0, 2, 0],
             [0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0]]).astype(float)
    elif g == 2:
        G = np.array([[0, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               [0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 0, 6, 0, 8, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 6, 0, 6, 5, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 3, 0],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]).astype(float)
    else:
        raise("No")
    n = np.size(G, 1)
    i_lower = np.tril_indices(n, -1)
    G[i_lower] = G.T[i_lower]
    return G

def WeightsStats(W):
    print("Weight statistics\n")
    mean = []
    median = []
    std = []
    min_w = []
    max_w = []
    n = []
    W_without_0_edges = np.copy(W[W!=0])
    mean.append(W_without_0_edges.mean())
    median.append(np.median(W_without_0_edges))
    std.append(W_without_0_edges.std())
    min_w.append(W_without_0_edges.min())
    max_w.append(W_without_0_edges.max())
    s = W_without_0_edges.shape[0]
    s = int((1+np.sqrt(1+4*2*s))/2)
    n.append(s)
    print(tabulate(list(zip(*[n, mean, median, std, min_w, max_w])), 
                   headers=["n", 'mean', 'median', 'std', "min_weight", "max_weight"], floatfmt=".3f"))
    return

def Get0Partition(n):
    partition = {}
    for i in range(n):
        partition[i]=0
    return partition

def Get1Partition(n):
    partition = {}
    for i in range(n):
        partition[i]=1
    return partition

def GetStupidPartition(n):
    partition = {}
    for i in range(n):
        partition[i]=-1
    return partition

def GetInitialPartition(n, k, initial_partition_type, seed = -1):
    if initial_partition_type == "random":
        initial_partition = GetKRandomPartition(n, k, seed)
    if initial_partition_type == "0":
        initial_partition = Get0Partition(n)
    if initial_partition_type == "1":
        initial_partition = Get1Partition(n)
    return initial_partition

def CutCost(W, partition, n, k):
    z = 0
    identity = np.identity(k)
    for i in range(n):
        for j in range(i, n):
            new_edge = (1-identity[partition[i], partition[j]])*W[i,j]
            z += new_edge
    return z
        
def CutGainAfterFlip(W, initial_partition, final_partition, n):
    # Vi: initial partition of the vertex v
    # Vj: final partition of the vertex v
    # z: current cut
    weights_to_add = 0
    weights_to_substract = 0
    
    diff_partition = {key: initial_partition[key] - final_partition.get(key, 0) for key in initial_partition}
    
    # v: vertex that flipped
    v = [key for key, val in diff_partition.items() if val!=0][0]
    v_old_partition = initial_partition[v]
    v_new_partition = final_partition[v]
    
    # Vertices belonging to the old group and the new group
    vertices_in_old_partition = [key  for (key, value) in initial_partition.items() if value == v_old_partition]
    vertices_in_new_partition = [key  for (key, value) in initial_partition.items() if value == v_new_partition]

    for vi in vertices_in_old_partition:
        if vi == v:
            continue
        else:
            weights_to_add += W[v, vi]
    for vj in vertices_in_new_partition:
        weights_to_substract += W[v, vj]
    z = weights_to_add - weights_to_substract
    return z
            
def SolveMaxCut(W, n, k, z, initial_partition, heuristic, seed = -1):
    tol = 1e-8
    maxIter = 100000
    it = 0
    
    zt1 = -10000
    zt2 = CutCost(W, initial_partition, n, k)
    
    # In case the cut is already optimal
    new_z = zt2
    
    # Store old and new partition
    old_partition = GetStupidPartition(n)
    new_partition = initial_partition
    
    start = time.time()
    if heuristic == "GBF":
        # Greedy Best Flip
        while(old_partition!=new_partition and it<maxIter):
            old_partition = new_partition
            new_partition, new_z = GreedyBestFlip(W, new_partition, n, k, zt2)
            zt1 = zt2
            zt2 = new_z
            if old_partition!=new_partition:
                it = it + 1
    elif heuristic == "RPF":
        # Random Positive Flip
         while(old_partition!=new_partition and it<maxIter):
            old_partition = new_partition
            new_partition, new_z = RandomPositiveFlip(W, new_partition, n, k, zt2, seed)
            zt1 = zt2
            zt2 = new_z
            if old_partition!=new_partition:
                it = it + 1
    elif heuristic == "WF":
        # Worst Flip
         while(old_partition!=new_partition and it<maxIter):
            old_partition = new_partition
            new_partition, new_z = WorstFlip(W, new_partition, n, k, zt2)
            zt1 = zt2
            zt2 = new_z
            if old_partition!=new_partition:
                it = it + 1

    elif heuristic == "FNF":
        # First Next Flip
        # Only for testing purposes
        iters_fnf = 3
        for i in range(iters_fnf):
            partition, new_z = FirstNextFlip(W, partition, n, k, zt2)
            zt1 = zt2
            zt2 = new_z
            if old_partition!=new_partition:
                it = it + 1
            
    end = time.time()
    elapsed_time = np.round(end - start, 4)
    # The last step would return the same partition, so it-1
    return new_partition, new_z, elapsed_time, it

def SymmetricMatrix(W):
    m = len(W)
    n = int((1+np.sqrt(1+4*2*m))/2)
    A = np.zeros(shape=(n, n))
    # k runs along W
    k = 0
    # Fill upper diagonal
    for i in range(n-1):
        for j in range(i+1, n):
            A[i,j]=W[k]
            k+=1
    # Fill lower diagonal
    i_lower = np.tril_indices(n, -1)
    A[i_lower] = A.T[i_lower]
    return A

def ScaleMatrix(W, a, b):
    non_zeros = np.nonzero(W)
    min_W = W[non_zeros].min()
    max_W = W.max()
    # Keep structure
    W_scaled = np.copy(W.astype(float))
    W_scaled[non_zeros] = (b-a)*((W[non_zeros]-min_W)/(max_W-min_W))+a
    return W_scaled

def NumpyToCsv(array, filename):
    np.savetxt(str(filename) + ".csv", array, delimiter=";")

def DeleteTempResults(filename):
    if os.path.exists(filename):
        os.remove(filename)
    return

def CsvToNumpy(filename):
    return np.genfromtxt(str(filename) + ".csv", delimiter=";")

def KolmogorovSmirnovTest(x1, x2, alpha):
    st, p = stats.ks_2samp(x1, x2)
    n1 = len(x1)
    n2 = len(x2)
    size_term = np.sqrt((n1+n2)/(n1*n2))
    if alpha == 0.05:
        d = size_term*1.36
    elif alpha == 0.025:
        d = size_term*1.48
    else:
        return
    if st < d:
        print("Both distributions are the same with probability", 100*(1-alpha), "%")
        print(st, "<", d)
    if st > d:
        print("Reject null hypothesis: different distributions with probability", 100*(1-alpha), "%")
        print(st, ">", d)
    return
    
# Meshgrid - No need to parallelize
def MeshGrid3dMaxCut(k, min_weight, max_weight, initial_partition_type, n_points_ax, n_iters, complexity, heuristic, sigma):
    weights_size = 3
    x = np.zeros(shape=(np.power(n_points_ax+1, 3), weights_size))
    f = np.zeros(np.power(n_points_ax+1, 3))
    # p runs over all points
    p = 0
    for i in range(n_points_ax+1):
        x_val = min_weight + (max_weight-min_weight)*i/n_points_ax
        for j in range(n_points_ax+1):
            y_val = min_weight + (max_weight-min_weight)*j/n_points_ax
            for l in range(n_points_ax+1):
                z_val = min_weight + (max_weight-min_weight)*l/n_points_ax
                w_flat = [x_val, y_val, z_val]
                w = SymmetricMatrix(w_flat)
                steps = np.zeros(n_iters)
                for it in range(n_iters):
                    initial_partition = GetInitialPartition(3, k, initial_partition_type)
                    initial_z = CutCost(w, initial_partition, 3, k)
                    if complexity == "average":
                        _p, _z, _t, steps[it] = SolveMaxCut(w, 3, k, initial_z, initial_partition, heuristic)
                    elif complexity == "smoothed":
                        # n_iters is also used for n_perturbations
                        steps[it] = SmoothedComplexity(w_flat, 3, k, initial_partition_type, 0, n_iters, heuristic, sigma)
                f[p] = np.mean(steps)
                x[p] = [x_val, y_val, z_val]
                p+=1
    return x, f

def PlotMeshGrid3d(x, f, title, colorsMap='jet'):
    cm = plt.get_cmap(colorsMap)
    cNorm = matcolors.Normalize(vmin=min(f), vmax=max(f))
    scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cm)
    fig = plt.figure()
    ax = Axes3D(fig)
    ax.scatter(x[:,0], x[:,1], x[:,2], c=scalarMap.to_rgba(f))
    plt.xticks(np.arange(min(x[:,0]), max(x[:,0])+1, 0.5))
    plt.yticks(np.arange(min(x[:,1]), max(x[:,1])+1, 0.5))
    scalarMap.set_array(f)
    fig.colorbar(scalarMap)
    if title:
        plt.title(title)
    plt.show()

# Plot results
def GetColorsVector(n):
    all_colors = ["blue", "red", "green", "black", "purple", "grey"]
    return all_colors[:n]

def PlotGraph(W, partition = None, ax=None):
    G = nx.from_numpy_matrix(np.round(W, 3))
    pos = nx.circular_layout(G)
    
    # Create edge labels
    labels = nx.get_edge_attributes(G, 'weight')
    edges, weights = zip(*nx.get_edge_attributes(G,'weight').items())
    
    # Draw the graph according to node positions
    if partition is not None:
        colors= np.fromiter(partition.values(), dtype=int)
        if ax is None:
            nx.draw(G, pos, with_labels=True, node_color=colors, edgelist=edges, edge_color=weights, width=5.0, edge_cmap=plt.cm.Blues)
        else:
            nx.draw(G, pos, with_labels=True, node_color=colors, edgelist=edges, edge_color=weights, width=5.0, edge_cmap=plt.cm.Blues, ax=ax)
    else:
        if ax is None:
            nx.draw(G, pos, with_labels=True, edgelist=edges, edge_color=weights, width=5.0, edge_cmap=plt.cm.Blues)
        else:
            nx.draw(G, pos, with_labels=True, edgelist=edges, edge_color=weights, width=5.0, edge_cmap=plt.cm.Blues, ax=ax)
    # Draw edge labels according to node positions
    #nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)
    if ax is None:
        plt.show()    
    return

def PlotWeightsHistogram(W):
    # Start plot
    plt.figure(1)
    plt.subplot(211)
    # Plot histogram of weights
    W_without_0_edges = np.copy(W[W!=0])
    plt.hist(W_without_0_edges, bins=20, color='blue')
    plt.title('Weight distribution', fontsize=8)
    # Do KSTest with uniform
    n = len(W)
    uniform_sample = np.random.uniform(low=-1, high=1, size=n)
    KolmogorovSmirnovTest(W, uniform_sample, alpha=0.025)
    # Histogram of the sum of the edges from each vertex
    sum_of_edges = np.sum(SymmetricMatrix(W), axis = 0)
    plt.subplot(212)
    plt.hist(sum_of_edges, bins=10)
    plt.title('Sum of edges', fontsize=8)
    plt.tight_layout()
    plt.show()
    # Do KSTest with artificial sample (assume independency)
    #artificial_sample = np.zeros(n)
    #for i in range(n):
    #    artificial_sample[i] = np.sum(np.random.uniform(low=-1, high=1, size=n-1))
    #KolmogorovSmirnovTest(sum_of_edges, artificial_sample, 0.05)
    return

def PlotRegressionResults(nodes, steps_mean, steps_sd, with_errorbars, r_var, r_var_label, method):
    
    # Convert X axis according to method
    x = nodes
    if method=="polynomial":
        x = np.log(x.reshape(-1, 1))
    if method == "quasipolynomial":
        x = np.log(x.reshape(-1, 1))*x.reshape(-1, 1)

    cmap = GetColorsVector(len(r_var))
    
    # t runs over all columns of steps_mean
    t=0
    slopes=[]
    intercepts=[]
    r_values=[]
    p_values=[]
    std_errs=[]

    # Create a new plot
    plt.figure(np.random.randint(501, 1000))
    
    for r_v in r_var:
        # Convert Y axis according to method
        y = steps_mean[:,t]
        if (method=="polynomial") or (method=="exponential") or (method=="quasipolynomial"):
            y_log = np.log(y)
        plt.scatter(x, y_log, c=cmap[t])
        if with_errorbars:
            y_sd = steps_sd[:, t]/y
            plt.errorbar(x,y_log,yerr=y_sd, linestyle="None", c=cmap[t])

        # Linear regression
        slope, intercept, r_value, p_value, std_err = stats.mstats.linregress(x,y_log)

        # Store results
        slopes.append(slope)
        intercepts.append(intercept)
        r_values.append(r_value)
        p_values.append(p_value)
        std_errs.append(std_err)

        # Linear regression prediction
        y_reg = intercept + slope*x
        plt.plot(x, y_reg, label=str(r_var_label) + '=' + str(r_v), c=cmap[t])
        t+=1

    if method=="polynomial":
        plt.title('Polynomial behavior')     
        plt.xlabel('ln(Nodes)') 
        plt.ylabel('ln(Steps)')
    
    if method=="exponential":
        plt.title('Exponential behavior')     
        plt.xlabel('Nodes') 
        plt.ylabel('ln(Steps)')
        
    if method=="quasipolynomial":
        plt.title('Quasiplynomial behavior')     
        plt.xlabel('ln(Nodes)*Nodes') 
        plt.ylabel('ln(Steps)')
    
    plt.legend()        
    plt.show()

    # Print table
    print("Regression results:\n")
    print(tabulate(list(zip(*[r_var, slopes, intercepts, r_values])), headers=[r_var_label, 'slope', 'intercept', 'r_value'], floatfmt=".3f"))
    
    return slopes, intercepts
    
def PlotResults(nodes, steps_mean, steps_sd, with_errorbars, r_var, r_var_label, xlabel = "Nodes", ylabel = "Steps"):
    #r_var is running variable
    
    # Show results for each r_var
    x = nodes.reshape(-1, 1)

    # Legend
    r_var_labels=[]

    # t runs over all columns of steps_mean
    t=0

    cmap = GetColorsVector(len(r_var))
    
    # Create a new plot
    plt.figure(np.random.randint(0, 500))
    
    for r_v in r_var:
        run_steps = steps_mean[:,t]
        plt.scatter(x, run_steps, c=cmap[t])
        if with_errorbars:
            run_steps_sd = steps_sd[:, t]
            plt.errorbar(x,run_steps,yerr=run_steps_sd, linestyle="None", c=cmap[t])
        r_var_labels.append(str(r_var_label) + "=" + str(r_v))
        t+=1

    plt.title(str(xlabel) + " vs. " + str(ylabel)) 
    plt.xlabel(xlabel) 
    plt.ylabel(ylabel)
    plt.legend(r_var_labels)
    plt.show()
    
def PlotResultsSmoothed(nodes, steps_mean, steps_sd, with_errorbars, r_var, r_var_label, sigma, withUpperBound):
    #r_var is running variable
    
    # Show results for each r_var
    x = nodes.reshape(-1, 1)

    # Legend
    r_var_labels=[]

    # t runs over all columns of steps_mean
    t=0
    
    # Create a new plot
    plt.figure(np.random.randint(1001, 1500))

    for r_v in r_var:
        run_steps = steps_mean[:,t]
        plt.scatter(x, run_steps)
        if with_errorbars:
            run_steps_sd = steps_sd[:, t]
            plt.errorbar(x,run_steps,yerr=run_steps_sd, linestyle="None")
        r_var_labels.append(str(r_var_label) + "=" + str(r_v))
        t+=1
    # Assume gaussian
    if withUpperBound:
        phi = 1/np.sqrt(2*np.pi*sigma*sigma)
        upper_bound = phi*np.power(nodes, 7.83)
        plt.plot(nodes, upper_bound, color='red', linewidth=1.0, linestyle='--')
    plt.title('Steps vs. Nodes') 
    plt.xlabel('Nodes') 
    plt.ylabel('Steps')
    plt.legend(r_var_labels)
    plt.show()
    return

def PlotTwoRegressionResults(nodes1, steps_mean1, steps_sd1, nodes2, steps_mean2, steps_sd2, with_errorbars, r_var, r_var_label, method, labels):
    
    # Create a new plot
    plt.figure(np.random.randint(501, 1000))

    # t runs over all columns of steps_mean
    t=0
    slopes1=[]
    intercepts1=[]
    r_values1=[]
    p_values1=[]
    std_errs1=[]    
    # Convert X axis according to method
    x1 = nodes1
    if method=="polynomial":
        x1 = np.log(x1.reshape(-1, 1))
    if method == "quasipolynomial":
        x1 = np.log(x1.reshape(-1, 1))*x1.reshape(-1, 1)
    
    # For the averae case, we have to reduce the number of vertices so that the have similar lengths
    last_index = np.argmax(nodes1>np.max(nodes2))
    if last_index == 0:
        last_index = len(x1-1)
    x1 = x1[:last_index]
    steps_mean1 = steps_mean1[:last_index]
    steps_sd1 = steps_sd1[:last_index]
    
    for r_v in r_var:
        # Convert Y axis according to method
        y1 = steps_mean1[:,t]
        if (method=="polynomial") or (method=="exponential") or (method=="quasipolynomial"):
            y1 = np.log(y1)
        plt.scatter(x1, y1, c = "darkred", alpha=0.5)
        if with_errorbars:
            if len(steps_sd1)>0:
                y_sd1 = steps_sd1[:, t]
                plt.errorbar(x1,y1,yerr=y_sd1, linestyle="None", c = "darkred", alpha=0.5)

        # Linear regression
        slope1, intercept1, r_value1, p_value1, std_err1 = stats.mstats.linregress(x1,y1)

        # Store results
        slopes1.append(slope1)
        intercepts1.append(intercept1)
        r_values1.append(r_value1)
        p_values1.append(p_value1)
        std_errs1.append(std_err1)

        # Linear regression prediction
        y_reg1 = intercept1 + slope1*x1
        plt.plot(x1, y_reg1, label=labels[0], c = "red")
        t+=1
        
    # t runs over all columns of steps_mean
    t=0
    slopes2=[]
    intercepts2=[]
    r_values2=[]
    p_values2=[]
    std_errs2=[]   
    
    # X2 is smoothed, therefore no error bars
    # Convert X axis according to method
    x2 = nodes2
    if method=="polynomial":
        x2 = np.log(x2.reshape(-1, 1))
    if method == "quasipolynomial":
        x2 = np.log(x2.reshape(-1, 1))*x2.reshape(-1, 1)
    
    for r_v in r_var:
        # Convert Y axis according to method
        y2 = steps_mean2[:,t]
        if (method=="polynomial") or (method=="exponential") or (method=="quasipolynomial"):
            y2 = np.log(y2)
        plt.scatter(x2, y2, c = "darkblue", alpha=0.5)
        if with_errorbars:
            if len(steps_sd2)>0:
                y_sd2 = steps_sd2[:, t]
                plt.errorbar(x2,y2,yerr=y_sd2, linestyle="None", c = "darkblue", alpha=0.5)

        # Linear regression
        slope2, intercept2, r_value2, p_value2, std_err2 = stats.mstats.linregress(x2,y2)

        # Store results
        slopes2.append(slope2)
        intercepts2.append(intercept2)
        r_values2.append(r_value2)
        p_values2.append(p_value2)
        std_errs2.append(std_err2)

        # Linear regression prediction
        y_reg2 = intercept2 + slope2*x2
        plt.plot(x2, y_reg2, label=labels[1], c = "blue")
        t+=1

    if method=="polynomial":
        plt.title('Polynomial behavior')     
        plt.xlabel('ln(Nodes)') 
        plt.ylabel('ln(Steps)')
    
    if method=="exponential":
        plt.title('Exponential behavior')     
        plt.xlabel('Nodes') 
        plt.ylabel('ln(Steps)')
        
    if method=="quasipolynomial":
        plt.title('Quasiplynomial behavior')     
        plt.xlabel('ln(Nodes)*Nodes') 
        plt.ylabel('ln(Steps)')
    
    plt.legend(loc="upper left")        
    plt.show()

    # Print table
    print("Regression results:\n")
    print(labels[0])
    print(tabulate(list(zip(*[r_var, slopes1, intercepts1, r_values1])), headers=[r_var_label, 'slope', 'intercept', 'r_value'], floatfmt=".3f"))
    print(labels[1])
    print(tabulate(list(zip(*[r_var, slopes2, intercepts2, r_values2])), headers=[r_var_label, 'slope', 'intercept', 'r_value'], floatfmt=".3f"))
    
    return

def MultilinearRegression(X, Y, method):
    import statsmodels.api as sm
    from sklearn import linear_model
    if method == "log-log":
        X = np.log(X)
        Y = np.log(Y)
    X = sm.add_constant(X) # adding a constant
    model = sm.OLS(Y, X).fit()
    #predictions = model.predict(X) 
    print_model = model.summary()
    print(print_model)
    return model.params, model.rsquared

def GreedyBestFlip(W, partition, n, k, z):
    cut_costs = []
    partitions_flip = []
    # Permutate vertices so that we do not always start by the same one
    order = np.random.permutation(range(n))
    for i in order:
        for j in range(1, k):            
            new_partition = partition.copy()
            new_partition[i] = (new_partition[i]+j)%k
            partitions_flip.append(new_partition)
            cut_costs.append(CutGainAfterFlip(W, partition, new_partition, n))
      
    # Convert the list to a numpy array
    cut_costs = np.asarray(cut_costs)
    if np.any(cut_costs[cut_costs>0]):
        # New best partition was found
        best_index = np.argmax(cut_costs)
        best_partition = partitions_flip[best_index]
        new_cut_cost = z + cut_costs[best_index]
        return best_partition, new_cut_cost
    else:
        # We are in a local optimum
        return partition, z
    
def RandomPositiveFlip(W, partition, n, k, z, seed = -1):
    # Permutate vertices so that we do not always start by the same one
    # Randomness
    if seed == -1:
        order = np.random.permutation(range(n))
    else:
        local_state = np.random.RandomState(seed)
        order = local_state.permutation(range(n))
    for i in order:
        for j in range(1, k):            
            new_partition = partition.copy()
            new_partition[i] = (new_partition[i]+j)%k
            new_z = CutGainAfterFlip(W, partition, new_partition, n)
            if new_z > 0:
                return new_partition, z + new_z
      
    return partition, z

def WorstFlip(W, partition, n, k, z):
    cut_costs = []
    partitions_flip = []
    # Permutate vertices so that we do not always start by the same one
    order = np.random.permutation(range(n))
    for i in order:
        for j in range(1, k):            
            new_partition = partition.copy()
            new_partition[i] = (new_partition[i]+j)%k
            partitions_flip.append(new_partition)
            cut_costs.append(CutGainAfterFlip(W, partition, new_partition, n))
      
    # Convert the list to a numpy array
    cut_costs = np.asarray(cut_costs)
    
    if np.any(cut_costs[cut_costs>0]):
        # A better partition was found
        min_val = min(c for c in cut_costs if c > 0)
        worst_index = np.argwhere(cut_costs==min_val)[0][0]
        worst_partition = partitions_flip[worst_index]
        new_cut_cost = z + cut_costs[worst_index]
        return worst_partition, new_cut_cost
    else:
        # We are in a local optimum
        return partition, z

from scipy.optimize import basinhopping
     
def SmoothedComplexity(W, n, k, initial_partition_type, dropout, iters_for_perturbation, heuristic, sigma):
    ####### IMPORTANT ########
    # It always returns the NEGATIVE smoothed complexity #
    steps = np.zeros(iters_for_perturbation)
    if np.asarray(W).ndim==1:
        W = np.copy(SymmetricMatrix(W))
    for it in range(iters_for_perturbation):
        # Create weight matrix
        W_k_it = np.copy(W)
        # Add perturbation
        if sigma > 0:
            # Keep structure! Do not perturbate dropouted edges!
            structure = np.nonzero(W_k_it)
            perturbation = np.random.normal(loc=0, scale=sigma, size=(n,n))
            W_k_it[structure] += perturbation[structure]
        # Make sure that the elements are in range (-1, 1)
        W_k_it[W_k_it > 1] = 1
        W_k_it[W_k_it < -1] = -1
        # Copy upper diagonal to lower diagonal (only the upper perturbation counts)
        i_lower = np.tril_indices(n, -1)
        W_k_it[i_lower] = np.copy(W_k_it.T[i_lower])
        # Make sure that diagonal is 0
        np.fill_diagonal(W_k_it, 0)
        # Get initial partition randomly
        initial_partition = GetInitialPartition(n, k, initial_partition_type)
        # Get initial cost value
        initial_z = CutCost(W_k_it, initial_partition, n, k)
        # Get next local maximum
        partition, z, elapsed_time, n_steps = SolveMaxCut(W_k_it, n, k, initial_z, initial_partition, heuristic)
        # Save results
        # times += elapsed_time
        steps[it]=n_steps
        # Think of another measure of the gap
    step_means = np.mean(steps)
    return -1*step_means

def OptimizationSmoothedComplexityK(min_nodes, max_nodes, step_nodes, initial_partition_type, ks, dropout, 
                                  min_weight, max_weight, n_perturbations, heuristic, sigma, n_iterations, algorithm, storeCSV=False):
    # Create nodes grid
    # nodes = np.linspace(min_nodes, max_nodes, num=step_nodes, dtype=int)
    nodes = np.logspace(np.log(min_nodes), np.log(max_nodes), num=step_nodes, base=np.exp(1))
    # Remove decimal part
    nodes = np.floor(nodes)
    # Convert to integer
    nodes = nodes.astype(int)
    # Store results
    smoothed_complexities = np.zeros((len(nodes), len(ks)))
    # Save nodes
    if storeCSV:
        NumpyToCsv(nodes, "nodes")
    
    # Running variables
    i = 0
    j = 0
    
    for ni in nodes:
        # Define weight size (upper diagonal)
        weights_size = int(ni*(ni-1)/2)
        # Create boundaries
        bound = (-1., 1.)
        bounds = []
        for m in range(weights_size):
            bounds.append(bound)
        bounded_step = RandomDisplacementBounds(np.array([b[0] for b in bounds]), np.array([b[1] for b in bounds]))
        for ki in ks:
            #Initial guess of size ni(ni-1)/2
            W0 = InitializeFlatGraph(weights_size, -1, 1, dropout)
            # Initialize kwargs
            if algorithm == "basinhopping":
                from scipy.optimize import basinhopping
                minimizer_kwargs = {"args": (ni, ki, initial_partition_type, dropout, n_perturbations, heuristic, sigma), "bounds": bounds}
                ret = basinhopping(SmoothedComplexity, W0, disp = False, minimizer_kwargs=minimizer_kwargs, niter_success=n_iterations, take_step=bounded_step)
            elif algorithm == "differential_evolution":
                minimizer_kwargs = [ni, ki, initial_partition_type, dropout, n_perturbations, heuristic, sigma]
                from scipy.optimize import differential_evolution
                ret = differential_evolution(SmoothedComplexity, bounds = bounds, disp = True, args=minimizer_kwargs, workers = 3)
            elif algorithm == "dual_annealing":
                minimizer_kwargs = [ni, ki, initial_partition_type, dropout, n_perturbations, heuristic, sigma]
                from scipy.optimize import dual_annealing
                ret = dual_annealing(SmoothedComplexity, bounds = bounds, args=minimizer_kwargs, maxiter=n_iterations)
            else: #local minimum
                minimizer_kwargs = (ni, ki, initial_partition_type, dropout, n_perturbations, heuristic, sigma)
                from scipy.optimize import minimize
                ret = minimize(SmoothedComplexity, W0, args=minimizer_kwargs, method=algorithm, bounds=bounds)
            smoothed_complexities[i, j] = ret.fun
            if storeCSV:
                DeleteTempResults("smoothed_complexities.csv")
                NumpyToCsv(smoothed_complexities, "smoothed_complexities")
            j = j + 1
        j = 0
        i = i + 1
    return nodes, -1*smoothed_complexities

def RandomGridSmoothedComplexityK(min_nodes, max_nodes, step_nodes, initial_partition_type, ks, dropout, 
                                  min_weight, max_weight, iters_for_nk, iters_for_w, heuristic, 
                                  sigma, plotMaxEvolution = False, plotWorstGraph = False, storeCSV=False):
    
    # Create nodes grid
    # nodes = np.linspace(min_nodes, max_nodes, num=step_nodes, dtype=int)
    nodes = np.logspace(np.log(min_nodes), np.log(max_nodes), num=step_nodes, base=np.exp(1))
    # Remove decimal part
    nodes = np.floor(nodes)
    # Convert to integer
    nodes = nodes.astype(int)
    # Store results
    step_means = np.zeros(iters_for_w)
    smoothed_complexities = np.zeros((len(nodes), len(ks)))
    # Save nodes
    if storeCSV:
        NumpyToCsv(nodes, "nodes")
       
    # Running variables
    i = 0
    j = 0
    
    for ni in nodes:
        # Define weight size (upper diagonal)
        weights_size = int(ni*(ni-1)/2)
        for ki in ks:
            best_results_so_far = np.zeros(iters_for_w)
            for w in range(iters_for_w):
                W = InitializeFlatGraph(weights_size, min_weight, max_weight, dropout)
                # Perform a dropout
                if dropout>0:
                    D = np.random.choice([0,1], size=weights_size, replace=True, p=[dropout, 1-dropout])
                    W = np.multiply(W,D)
                structure = np.nonzero(W)
                if plotWorstGraph and w == 0:
                    print("First graph\n")
                    WeightsStats(W)
                    PlotGraph(SymmetricMatrix(W))
                    PlotWeightsHistogram(W)
                # Initialize results
                steps = np.zeros(iters_for_nk)
                for nk in range(iters_for_nk):
                    # Create perturbation
                    perturbation = np.random.normal(loc=0, scale=sigma, size=weights_size)
                    W_k_it = np.copy(W)
                    W_k_it[structure] = W_k_it[structure] + perturbation[structure]        
                    # Convert to symmetric matrix
                    W_k_it = SymmetricMatrix(W_k_it)
                    # Get initial partition randomly
                    initial_partition = GetInitialPartition(ni, ki, initial_partition_type)
                    # Get initial cost value
                    initial_z = CutCost(W_k_it, initial_partition, ni, ki)
                    # Get next local maximum
                    partition, z, elapsed_time, n_steps = SolveMaxCut(W_k_it, ni, ki, initial_z, initial_partition, heuristic)
                    # Save results
                    # times += elapsed_time
                    steps[nk]=n_steps
                    # Think of another measure of the gap
                step_means[w] = np.mean(steps)
                # Store values for plotResults
                
                if w == 0:
                    best_results_so_far[0] = step_means[0]
                    worstGraph = W
                else:
                    best_results_so_far[w] = max(step_means[w], best_results_so_far[w-1])
                    if step_means[w]>best_results_so_far[w-1]:
                        worstGraph = W
            # Plot results if required
            if plotMaxEvolution:
                PlotResults(np.asarray(range(iters_for_w)), np.reshape(best_results_so_far, (iters_for_w, 1)), [], 
                            False, [ni], "n", "Iter", "S.C.")
                PlotResults(np.asarray(range(iters_for_w)), np.reshape(step_means, (iters_for_w, 1)), [], 
                            False, [ni], "n", "Iter", "A.C.")
            if plotWorstGraph:
                print("Worst graph\n")
                WeightsStats(worstGraph)
                PlotGraph(SymmetricMatrix(worstGraph))
                PlotWeightsHistogram(worstGraph)
                
            smoothed_complexities[i,j] = np.max(step_means)
            print("Best result for n=" + str(ni) + " and k=" + str(ki) + " is " + str(smoothed_complexities[i,j]))
            if storeCSV:
                DeleteTempResults("smoothed_complexities.csv")
                NumpyToCsv(smoothed_complexities, "smoothed_complexities")
            j = j + 1
        i = i + 1
        j = 0        
    
    return nodes, smoothed_complexities

def RandomGridSmoothedComplexityDropout(min_nodes, max_nodes, step_nodes, initial_partition_type, k, dropouts, 
                                  min_weight, max_weight, iters_for_nk, iters_for_w, heuristic, 
                                  sigma, plotMaxEvolution = False, plotWorstGraph = False, storeCSV=False):
    
    # Create nodes grid
    # nodes = np.linspace(min_nodes, max_nodes, num=step_nodes, dtype=int)
    nodes = np.logspace(np.log(min_nodes), np.log(max_nodes), num=step_nodes, base=np.exp(1))
    # Remove decimal part
    nodes = np.floor(nodes)
    # Convert to integer
    nodes = nodes.astype(int)
    # Store results
    step_means = np.zeros(iters_for_w)
    smoothed_complexities = np.zeros((len(nodes), len(dropouts)))
    # Save nodes
    if storeCSV:
        NumpyToCsv(nodes, "nodes")
       
    # Running variables
    i = 0
    j = 0
    
    for ni in nodes:
        # Define weight size (upper diagonal)
        weights_size = int(ni*(ni-1)/2)
        for dropout in dropouts:
            best_results_so_far = np.zeros(iters_for_w)
            for w in range(iters_for_w):
                W = InitializeFlatGraph(weights_size, min_weight, max_weight, dropout)
                # Perform a dropout
                if dropout>0:
                    D = np.random.choice([0,1], size=weights_size, replace=True, p=[dropout, 1-dropout])
                    W = np.multiply(W,D)
                structure = np.nonzero(W)
                if plotWorstGraph and w == 0:
                    print("First graph\n")
                    WeightsStats(W)
                    PlotGraph(SymmetricMatrix(W))
                    PlotWeightsHistogram(W)
                # Initialize results
                steps = np.zeros(iters_for_nk)
                for nk in range(iters_for_nk):
                    # Create perturbation
                    perturbation = np.random.normal(loc=0, scale=sigma, size=weights_size)
                    W_k_it = np.copy(W)
                    W_k_it[structure] = W_k_it[structure] + perturbation[structure]        
                    # Convert to symmetric matrix
                    W_k_it = SymmetricMatrix(W_k_it)
                    # Get initial partition randomly
                    initial_partition = GetInitialPartition(ni, k, initial_partition_type)
                    # Get initial cost value
                    initial_z = CutCost(W_k_it, initial_partition, ni, k)
                    # Get next local maximum
                    partition, z, elapsed_time, n_steps = SolveMaxCut(W_k_it, ni, k, initial_z, initial_partition, heuristic)
                    # Save results
                    # times += elapsed_time
                    steps[nk]=n_steps
                    # Think of another measure of the gap
                step_means[w] = np.mean(steps)
                # Store values for plotResults
                
                if w == 0:
                    best_results_so_far[0] = step_means[0]
                    worstGraph = W
                else:
                    best_results_so_far[w] = max(step_means[w], best_results_so_far[w-1])
                    if step_means[w]>best_results_so_far[w-1]:
                        worstGraph = W
            # Plot results if required
            if plotMaxEvolution:
                PlotResults(np.asarray(range(iters_for_w)), np.reshape(best_results_so_far, (iters_for_w, 1)), [], 
                            False, [ni], "n", "Iter", "S.C.")
            if plotWorstGraph:
                print("Worst graph\n")
                WeightsStats(worstGraph)
                PlotGraph(SymmetricMatrix(worstGraph))
                PlotWeightsHistogram(worstGraph)
                
            smoothed_complexities[i,j] = np.max(step_means)
            print("Best result for n=" + str(ni) + " and dropout=" + str(dropout) + " is " + str(smoothed_complexities[i,j]))
            if storeCSV:
                DeleteTempResults("smoothed_complexities.csv")
                NumpyToCsv(smoothed_complexities, "smoothed_complexities")
            j = j + 1
        i = i + 1
        j = 0        
    
    return nodes, smoothed_complexities

def RandomGridSmoothedComplexityHeuristic(min_nodes, max_nodes, step_nodes, initial_partition_type, k, dropout, 
                                  min_weight, max_weight, iters_for_nk, iters_for_w, heuristics, 
                                  sigma, plotMaxEvolution = False, plotWorstGraph = False, storeCSV=False):
    
    # Create nodes grid
    # nodes = np.linspace(min_nodes, max_nodes, num=step_nodes, dtype=int)
    nodes = np.logspace(np.log(min_nodes), np.log(max_nodes), num=step_nodes, base=np.exp(1))
    # Remove decimal part
    nodes = np.floor(nodes)
    # Convert to integer
    nodes = nodes.astype(int)
    # Store results
    step_means = np.zeros(iters_for_w)
    smoothed_complexities = np.zeros((len(nodes), len(heuristics)))
    # Save nodes
    if storeCSV:
        NumpyToCsv(nodes, "nodes")
       
    # Running variables
    i = 0
    j = 0
    
    for ni in nodes:
        # Define weight size (upper diagonal)
        weights_size = int(ni*(ni-1)/2)
        for heuristic in heuristics:
            best_results_so_far = np.zeros(iters_for_w)
            for w in range(iters_for_w):
                W = InitializeFlatGraph(weights_size, min_weight, max_weight, dropout)
                # Perform a dropout
                if dropout>0:
                    D = np.random.choice([0,1], size=weights_size, replace=True, p=[dropout, 1-dropout])
                    W = np.multiply(W,D)
                structure = np.nonzero(W)
                if plotWorstGraph and w == 0:
                    print("First graph\n")
                    WeightsStats(W)
                    PlotGraph(SymmetricMatrix(W))
                    PlotWeightsHistogram(W)
                # Initialize results
                steps = np.zeros(iters_for_nk)
                for nk in range(iters_for_nk):
                    # Create perturbation
                    perturbation = np.random.normal(loc=0, scale=sigma, size=weights_size)
                    W_k_it = np.copy(W)
                    W_k_it[structure] = W_k_it[structure] + perturbation[structure]        
                    # Convert to symmetric matrix
                    W_k_it = SymmetricMatrix(W_k_it)
                    # Get initial partition randomly
                    initial_partition = GetInitialPartition(ni, k, initial_partition_type)
                    # Get initial cost value
                    initial_z = CutCost(W_k_it, initial_partition, ni, k)
                    # Get next local maximum
                    partition, z, elapsed_time, n_steps = SolveMaxCut(W_k_it, ni, k, initial_z, initial_partition, heuristic)
                    # Save results
                    # times += elapsed_time
                    steps[nk]=n_steps
                    # Think of another measure of the gap
                step_means[w] = np.mean(steps)
                # Store values for plotResults
                
                if w == 0:
                    best_results_so_far[0] = step_means[0]
                    worstGraph = W
                else:
                    best_results_so_far[w] = max(step_means[w], best_results_so_far[w-1])
                    if step_means[w]>best_results_so_far[w-1]:
                        worstGraph = W
            # Plot results if required
            if plotMaxEvolution:
                PlotResults(np.asarray(range(iters_for_w)), np.reshape(best_results_so_far, (iters_for_w, 1)), [], 
                            False, [ni], "n", "Iter", "S.C.")
            if plotWorstGraph:
                print("Worst graph\n")
                WeightsStats(worstGraph)
                PlotGraph(SymmetricMatrix(worstGraph))
                PlotWeightsHistogram(worstGraph)
                
            smoothed_complexities[i,j] = np.max(step_means)
            print("Best result for n=" + str(ni) + " and heuristic=" + str(heuristic) + " is " + str(smoothed_complexities[i,j]))
            if storeCSV:
                DeleteTempResults("smoothed_complexities.csv")
                NumpyToCsv(smoothed_complexities, "smoothed_complexities")
            j = j + 1
        i = i + 1
        j = 0        
    
    return nodes, smoothed_complexities

def RandomGridSmoothedComplexitySigma(min_nodes, max_nodes, step_nodes, initial_partition_type, k, dropout, 
                                  min_weight, max_weight, iters_for_nk, iters_for_w, heuristic, 
                                  sigmas, plotMaxEvolution = False, plotWorstGraph = False, storeCSV=False):
    
    # Create nodes grid
    # nodes = np.linspace(min_nodes, max_nodes, num=step_nodes, dtype=int)
    nodes = np.logspace(np.log(min_nodes), np.log(max_nodes), num=step_nodes, base=np.exp(1))
    # Remove decimal part
    nodes = np.floor(nodes)
    # Convert to integer
    nodes = nodes.astype(int)
    # Store results
    step_means = np.zeros(iters_for_w)
    smoothed_complexities = np.zeros((len(nodes), len(sigmas)))
    # Save nodes
    if storeCSV:
        NumpyToCsv(nodes, "nodes")
       
    # Running variables
    i = 0
    j = 0
    
    for ni in nodes:
        # Define weight size (upper diagonal)
        weights_size = int(ni*(ni-1)/2)
        for sigma in sigmas:
            best_results_so_far = np.zeros(iters_for_w)
            for w in range(iters_for_w):
                W = InitializeFlatGraph(weights_size, min_weight, max_weight, dropout)
                # Perform a dropout
                if dropout>0:
                    D = np.random.choice([0,1], size=weights_size, replace=True, p=[dropout, 1-dropout])
                    W = np.multiply(W,D)
                structure = np.nonzero(W)
                if plotWorstGraph and w == 0:
                    print("First graph\n")
                    WeightsStats(W)
                    PlotGraph(SymmetricMatrix(W))
                    PlotWeightsHistogram(W)
                # Initialize results
                steps = np.zeros(iters_for_nk)
                for nk in range(iters_for_nk):
                    # Create perturbation
                    perturbation = np.random.normal(loc=0, scale=sigma, size=weights_size)
                    W_k_it = np.copy(W)
                    W_k_it[structure] = W_k_it[structure] + perturbation[structure]        
                    # Convert to symmetric matrix
                    W_k_it = SymmetricMatrix(W_k_it)
                    # Get initial partition randomly
                    initial_partition = GetInitialPartition(ni, k, initial_partition_type)
                    # Get initial cost value
                    initial_z = CutCost(W_k_it, initial_partition, ni, k)
                    # Get next local maximum
                    partition, z, elapsed_time, n_steps = SolveMaxCut(W_k_it, ni, k, initial_z, initial_partition, heuristic)
                    # Save results
                    # times += elapsed_time
                    steps[nk]=n_steps
                    # Think of another measure of the gap
                step_means[w] = np.mean(steps)
                # Store values for plotResults
                
                if w == 0:
                    best_results_so_far[0] = step_means[0]
                    worstGraph = W
                else:
                    best_results_so_far[w] = max(step_means[w], best_results_so_far[w-1])
                    if step_means[w]>best_results_so_far[w-1]:
                        worstGraph = W
            # Plot results if required
            if plotMaxEvolution:
                PlotResults(np.asarray(range(iters_for_w)), np.reshape(best_results_so_far, (iters_for_w, 1)), [], 
                            False, [ni], "n", "Iter", "S.C.")
            if plotWorstGraph:
                print("Worst graph\n")
                WeightsStats(worstGraph)
                PlotGraph(SymmetricMatrix(worstGraph))
                PlotWeightsHistogram(worstGraph)
                
            smoothed_complexities[i,j] = np.max(step_means)
            print("Best result for n=" + str(ni) + " and sigma=" + str(sigma) + " is " + str(smoothed_complexities[i,j]))
            if storeCSV:
                DeleteTempResults("smoothed_complexities.csv")
                NumpyToCsv(smoothed_complexities, "smoothed_complexities")
            j = j + 1
        i = i + 1
        j = 0        
    
    return nodes, smoothed_complexities

def RandomGridSmoothedComplexityIntercalations(min_nodes, max_nodes, step_nodes, initial_partition_type, k, dropout, 
                                  min_weight, max_weight, iters_for_nk, iters_for_w, heuristic, 
                                  sigma, intercalations, plotMaxEvolution = False, plotWorstGraph = False, storeCSV=False):
    
    # Create nodes grid
    # nodes = np.linspace(min_nodes, max_nodes, num=step_nodes, dtype=int)
    nodes = np.logspace(np.log(min_nodes), np.log(max_nodes), num=step_nodes, base=np.exp(1))
    # Remove decimal part
    nodes = np.floor(nodes)
    # Convert to integer
    nodes = nodes.astype(int)
    # Store results
    step_means = np.zeros(iters_for_w)
    smoothed_complexities = np.zeros((len(nodes), len(intercalations)))
    # Save nodes
    if storeCSV:
        NumpyToCsv(nodes, "nodes")
       
    # Running variables
    i = 0
    j = 0
    
    for ni in nodes:
        # Define weight size (upper diagonal)
        weights_size = int(ni*(ni-1)/2)
        for intercalation in intercalations:
            best_results_so_far = np.zeros(iters_for_w)
            real_intercalation = np.floor(intercalation*ni).astype(int)
            for w in range(iters_for_w):
                W = InitializeFlatGraph(ni, min_weight, max_weight, dropout, None, "intercalate", real_intercalation)
                # Perform a dropout
                if dropout>0:
                    D = np.random.choice([0,1], size=weights_size, replace=True, p=[dropout, 1-dropout])
                    W = np.multiply(W,D)
                structure = np.nonzero(W)
                if plotWorstGraph and w == 0:
                    print("First graph\n")
                    WeightsStats(W)
                    PlotGraph(SymmetricMatrix(W))
                    PlotWeightsHistogram(W)
                # Initialize results
                steps = np.zeros(iters_for_nk)
                for nk in range(iters_for_nk):
                    # Create perturbation
                    perturbation = np.random.normal(loc=0, scale=sigma, size=weights_size)
                    W_k_it = np.copy(W)
                    W_k_it[structure] = W_k_it[structure] + perturbation[structure]        
                    # Convert to symmetric matrix
                    W_k_it = SymmetricMatrix(W_k_it)
                    # Get initial partition randomly
                    initial_partition = GetInitialPartition(ni, k, initial_partition_type)
                    # Get initial cost value
                    initial_z = CutCost(W_k_it, initial_partition, ni, k)
                    # Get next local maximum
                    partition, z, elapsed_time, n_steps = SolveMaxCut(W_k_it, ni, k, initial_z, initial_partition, heuristic)
                    # Save results
                    # times += elapsed_time
                    steps[nk]=n_steps
                    # Think of another measure of the gap
                step_means[w] = np.mean(steps)
                # Store values for plotResults
                
                if w == 0:
                    best_results_so_far[0] = step_means[0]
                    worstGraph = W
                else:
                    best_results_so_far[w] = max(step_means[w], best_results_so_far[w-1])
                    if step_means[w]>best_results_so_far[w-1]:
                        worstGraph = W
            # Plot results if required
            if plotMaxEvolution:
                PlotResults(np.asarray(range(iters_for_w)), np.reshape(best_results_so_far, (iters_for_w, 1)), [], 
                            False, [ni], "n", "Iter", "S.C.")
            if plotWorstGraph:
                print("Worst graph\n")
                WeightsStats(worstGraph)
                PlotGraph(SymmetricMatrix(worstGraph))
                PlotWeightsHistogram(worstGraph)
                
            smoothed_complexities[i,j] = np.max(step_means)
            print("Best result for n=" + str(ni) + " and intercalation=" + str(intercalation) + " is " + str(smoothed_complexities[i,j]))
            if storeCSV:
                DeleteTempResults("smoothed_complexities.csv")
                NumpyToCsv(smoothed_complexities, "smoothed_complexities")
            j = j + 1
        i = i + 1
        j = 0        
    
    return nodes, smoothed_complexities

def RandomGridSmoothedComplexityParallelK(min_nodes, max_nodes, step_nodes, initial_partition_type, ks, dropout, 
                                  min_weight, max_weight, n_perturbations, tries, heuristic, sigma):
    
    # Create nodes grid
    # nodes = np.linspace(min_nodes, max_nodes, num=step_nodes, dtype=int)
    nodes = np.logspace(np.log(min_nodes), np.log(max_nodes), num=step_nodes, base=np.exp(1))
    # Remove decimal part
    nodes = np.floor(nodes)
    # Convert to integer
    nodes = nodes.astype(int)
    # Store results
    smoothed_complexities = np.zeros((len(nodes), len(ks)))
       
    # Running variables
    i = 0
    j = 0
    best_result = 0
    
    for ni in nodes:
        # Define weight size (upper diagonal)
        weights_size = int(ni*(ni-1)/2)
        for ki in ks:
            start = time.time()
            N = mp.cpu_count()
	    print(N, "processors")
            iters_per_processor = np.floor(tries/N)
            if (__name__ == '__main__'):       
                # Define an output queue
                output = mp.Manager().Queue()
                processes = [mp.Process(target=RunRandomSmoothedComplexity, 
                                        args=(ni, min_weight, max_weight, dropout, initial_partition_type,
                                              ki, iters_per_processor, n_perturbations, heuristic, sigma, output)) 
                              for n_p in range(int(N))]
                # Start processes
                for p in processes:
                    p.start()

                # Exit the completed processes
                for p in processes:
                    p.join()

                # Get process results from the output queue
                results = [output.get() for p in processes]
                results_values = []
                for l in range(len(results)):
                    results_values.append(results[l][-1])

            iter_result = np.max(results_values)
            best_index = np.argmax(results_values)
            end = time.time()
            if iter_result > best_result:
                best_result = iter_result
    
            smoothed_complexities[i,j] = best_result
            
            print("Best result for n=" + str(ni) + " and k=" + str(ki) + " is " + str(best_result))
            end = time.time()
            print("Elapsed time: " + str(np.round((end-start), 2)) + " s.")
            best_result = 0
            j = j + 1
        i = i + 1
        j = 0
    
    return nodes, smoothed_complexities

def RandomSearchSmoothedComplexitySigmas(min_nodes, max_nodes, step_nodes, initial_partition_type, k, dropout, 
                                  min_weight, max_weight, iters_for_nk, iters_for_w, heuristic, sigmas):
    
    # Create nodes grid
    # nodes = np.linspace(min_nodes, max_nodes, num=step_nodes, dtype=int)
    nodes = np.logspace(np.log(min_nodes), np.log(max_nodes), num=step_nodes, base=np.exp(1))
    # Remove decimal part
    nodes = np.floor(nodes)
    # Convert to integer
    nodes = nodes.astype(int)
    # Store results
    step_means = np.zeros(iters_for_w)
    smoothed_complexities = np.zeros((len(nodes), len(sigmas)))
       
    # Running variables
    i = 0
    j = 0
    
    for ni in nodes:
        # Define weight size (upper diagonal)
        weights_size = int(ni*(ni-1)/2)
        for s in sigmas:
            for w in range(iters_for_w):
                W = InitializeFlatGraph(weights_size, min_weight, max_weight, dropout)
                # Perform a dropout
                if dropout>0:
                    D = np.random.choice([0,1], size=weights_size, replace=True, p=[dropout, 1-dropout])
                    W = np.multiply(W,D)
                # Keep structure
                structure = np.nonzero(W)
                # Initialize results
                steps = np.zeros(iters_for_nk)
                for nk in range(iters_for_nk):
                    # Create perturbation
                    perturbation = np.random.normal(loc=0, scale=s, size=weights_size)
                    W_k_it = np.copy(W)
                    W_k_it[structure] = W_k_it[structure] + perturbation[structure]                    
                    # Convert to symmetric matrix
                    W_k_it = SymmetricMatrix(W_k_it)
                    # Get initial partition randomly
                    initial_partition = GetInitialPartition(ni, k, initial_partition_type)
                    # Get initial cost value
                    initial_z = CutCost(W_k_it, initial_partition, ni, k)
                    # Get next local maximum
                    partition, z, elapsed_time, n_steps = SolveMaxCut(W_k_it, ni, k, initial_z, initial_partition, heuristic)
                    # Save results
                    # times += elapsed_time
                    steps[nk]=n_steps
                    # Think of another measure of the gap
                step_means[w] = np.mean(steps)
            smoothed_complexities[i,j] = np.max(step_means)
            j = j + 1
        i = i + 1
        j = 0
    
    return nodes, smoothed_complexities

def RandomSearchSmoothedComplexityDropout(min_nodes, max_nodes, step_nodes, initial_partition_type, k, dropouts, 
                                  min_weight, max_weight, iters_for_nk, iters_for_w, heuristic, sigma):
    
    # Create nodes grid
    # nodes = np.linspace(min_nodes, max_nodes, num=step_nodes, dtype=int)
    nodes = np.logspace(np.log(min_nodes), np.log(max_nodes), num=step_nodes, base=np.exp(1))
    # Remove decimal part
    nodes = np.floor(nodes)
    # Convert to integer
    nodes = nodes.astype(int)
    # Store results
    step_means = np.zeros(iters_for_w)
    smoothed_complexities = np.zeros((len(nodes), len(dropouts)))
       
    # Running variables
    i = 0
    j = 0
    
    for ni in nodes:
        # Define weight size (upper diagonal)
        weights_size = int(ni*(ni-1)/2)
        for d in dropouts:
            for w in range(iters_for_w):
                W = InitializeFlatGraph(weights_size, min_weight, max_weight, dropout)
                # Perform a dropout
                if d>0:
                    D = np.random.choice([0,1], size=weights_size, replace=True, p=[d, 1-d])
                    W = np.multiply(W,D)
                # Keep structure
                structure = np.nonzero(W)
                # Initialize results
                steps = np.zeros(iters_for_nk)
                for nk in range(iters_for_nk):
                    # Perturbate real edges
                    perturbation = np.random.normal(loc=0, scale=sigma, size=weights_size)
                    W_k_it = np.copy(W)
                    W_k_it[structure] = W_k_it[structure] + perturbation[structure]                    
                    # Convert to symmetric matrix
                    W_k_it = SymmetricMatrix(W_k_it)
                    # Get initial partition randomly
                    initial_partition = GetInitialPartition(ni, k, initial_partition_type)
                    # Get initial cost value
                    initial_z = CutCost(W_k_it, initial_partition, ni, k)
                    # Get next local maximum
                    partition, z, elapsed_time, n_steps = SolveMaxCut(W_k_it, ni, k, initial_z, initial_partition, heuristic)
                    # Save results
                    # times += elapsed_time
                    steps[nk]=n_steps
                    # Think of another measure of the gap
                step_means[w] = np.mean(steps)
            smoothed_complexities[i,j] = np.max(step_means)
            j = j + 1
        i = i + 1
        j = 0
    
    return nodes, smoothed_complexities

def RandomSearchSmoothedComplexityHeuristics(min_nodes, max_nodes, step_nodes, initial_partition_type, k, dropout, 
                                  min_weight, max_weight, iters_for_nk, iters_for_w, heuristics, sigma):
    
    # Create nodes grid
    # nodes = np.linspace(min_nodes, max_nodes, num=step_nodes, dtype=int)
    nodes = np.logspace(np.log(min_nodes), np.log(max_nodes), num=step_nodes, base=np.exp(1))
    # Remove decimal part
    nodes = np.floor(nodes)
    # Convert to integer
    nodes = nodes.astype(int)
    # Store results
    step_means = np.zeros(iters_for_w)
    smoothed_complexities = np.zeros((len(nodes), len(heuristics)))
       
    # Running variables
    i = 0
    j = 0
    
    for ni in nodes:
        # Define weight size (upper diagonal)
        weights_size = int(ni*(ni-1)/2)
        for h in heuristics:
            for w in range(iters_for_w):
                W = InitializeFlatGraph(weights_size, min_weight, max_weight, dropout)
                # Perform a dropout
                if dropout>0:
                    D = np.random.choice([0,1], size=weights_size, replace=True, p=[dropout, 1-dropout])
                    W = np.multiply(W,D)
                structure = np.nonzero(W)
                # Initialize results
                steps = np.zeros(iters_for_nk)
                for nk in range(iters_for_nk):
                    # Create perturbation
                    perturbation = np.random.normal(loc=0, scale=sigma, size=weights_size)
                    W_k_it = np.copy(W)
                    W_k_it[structure] = W_k_it[structure] + perturbation[structure]
                    # Convert to symmetric matrix
                    W_k_it = SymmetricMatrix(W_k_it)
                    # Get initial partition randomly
                    initial_partition = GetInitialPartition(ni, k, initial_partition_type)
                    # Get initial cost value
                    initial_z = CutCost(W_k_it, initial_partition, ni, k)
                    # Get next local maximum
                    partition, z, elapsed_time, n_steps = SolveMaxCut(W_k_it, ni, k, initial_z, initial_partition, h)
                    # Save results
                    # times += elapsed_time
                    steps[nk]=n_steps
                    # Think of another measure of the gap
                step_means[w] = np.mean(steps)
            smoothed_complexities[i,j] = np.max(step_means)
            j = j + 1
        i = i + 1
        j = 0
    
    return nodes, smoothed_complexities

def DecayLearningRate(e0, decay, iters):
    e = e0
    iters = iters + 1
    e = e0/(1+decay*iters)
    return e

def RandomOptimizationSmoothedComplexityK(min_nodes, max_nodes, step_nodes, initial_partition_type, ks, 
                                  min_weight, max_weight, iters, tries, heuristic, iters_for_perturbation, e0, decay, sigma):
    # Create nodes grid
    # nodes = np.linspace(min_nodes, max_nodes, num=step_nodes, dtype=int)
    nodes = np.logspace(np.log(min_nodes), np.log(max_nodes), num=step_nodes, base=np.exp(1))
    # Remove decimal part
    nodes = np.floor(nodes)
    # Convert to integer
    nodes = nodes.astype(int)
    # Store results
    smoothed_complexities = np.zeros((len(nodes), len(ks)))
       
    # Running variables
    i = 0
    j = 0

    # Define first learning rate
    
    best_result = 0

    for ni in nodes:
        # Define weight size (upper diagonal)
        weights_size = int(ni*(ni-1)/2)
        for ki in ks:
            # First guess
            x = InitializeFlatGraph(weights_size, min_weight, max_weight, dropout)
            e = e0
            for it in range(iters):          
                for t in range(tries):
                    # Create perturbation
                    perturbation = np.random.normal(loc=0, scale=e, size=weights_size)
                    x_t = np.copy(x) + perturbation
                    # Make sure elements stay within range
                    x_t[x_t > 1] = 1
                    x_t[x_t < -1] = -1
                    # Convert to symmetric matrix
                    W_k_it = SymmetricMatrix(x_t)
                    # Get initial partition randomly
                    initial_partition = GetInitialPartition(ni, ki, initial_partition_type)
                    # Get initial cost value
                    initial_z = CutCost(W_k_it, initial_partition, ni, ki)
                    # Get next local maximum
                    result = -1*(SmoothedComplexity(x_t, ni, ki, initial_partition_type, 0, iters_for_perturbation, heuristic, sigma))
                    # Save results
                    if result>best_result:
                        best_result = result
                        best_x = x_t
                e = DecayLearningRate(e0, decay, iters)
            smoothed_complexities[i,j] = best_result
            print("Best result for n=" + str(ni) + " and k=" + str(ki) + " is " + str(best_result))
            best_result = 0
            j = j + 1
        i = i + 1
        j = 0
    
    return nodes, smoothed_complexities 

def RandomOptimizationSmoothedComplexityParallelK(min_nodes, max_nodes, step_nodes, initial_partition_type, ks, 
                                  min_weight, max_weight, iters_max, tries, heuristic, iters_for_perturbation, e0, decay, sigma):
    # Create nodes grid
    # nodes = np.linspace(min_nodes, max_nodes, num=step_nodes, dtype=int)
    nodes = np.logspace(np.log(min_nodes), np.log(max_nodes), num=step_nodes, base=np.exp(1))
    # Remove decimal part
    nodes = np.floor(nodes)
    # Convert to integer
    nodes = nodes.astype(int)
    # Store results
    smoothed_complexities = np.zeros((len(nodes), len(ks)))
       
    # Running variables
    i = 0
    j = 0

    # Tolerance
    tol = 1e-4

    # Initiliaze
    best_result = 0
    e = e0
    
    for ni in nodes:
        # Define weight size (upper diagonal)
        weights_size = int(ni*(ni-1)/2)
        for ki in ks:
            # First guess
            x = InitializeFlatGraph(weights_size, min_weight, max_weight, dropout)
            it = 0
            start = time.time()
            while(it<iters_max):
                print("It " + str(it))
                #N = tf.contrib.eager.num_gpus()
                N = mp.cpu_count()
                #print("N of cpu count = ")
                #print(N)
                if (__name__ == '__main__'):
                    
                    # Define an output queue
                    output = mp.Manager().Queue()
                    processes = [mp.Process(target=RunSmoothedComplexity, 
                                            args=(x, ni, initial_partition_type, ki, e, iters_for_perturbation, heuristic, sigma, t, output)) 
                                 for t in range(tries)]
                    # Start processes
                    for p in processes:
                        p.start()

                    # Exit the completed processes
                    for p in processes:
                        p.join()

                    # Get process results from the output queue
                    results = [output.get() for p in processes]
                    results_values = []
                    results_x = []
                    for l in range(len(results)):
                        results_values.append(results[l][-1])
                        results_x.append(results[l][:-1])

                iter_result = np.max(results_values)
                best_index = np.argmax(results_values)
                end = time.time
                if iter_result > best_result:
                    best_result = iter_result
                    x = results_x[best_index]
                    e = DecayLearningRate(e0, decay, it)
                else:
                  break
                it+=1
                
            smoothed_complexities[i,j] = best_result
            print("Best result for n=" + str(ni) + " and k=" + str(ki) + " is " + str(best_result))
            end = time.time()
            print("Elapsed time: " + str(np.round((end-start), 2)) + " s.")
            best_result = 0            
            j = j + 1
        i = i + 1
        j = 0
    
    return nodes, smoothed_complexities
        
def RunSmoothedComplexity(x, n, initial_partition_type, k, e, iters_for_perturbation, heuristic, sigma, seed, output):
    # Weights size is the length of x
    weights_size = len(x)
    # Create perturbation
    local_state = np.random.RandomState(seed)
    structure = np.nonzero(x)
    # Store weights
    x_t = np.copy(x)
    if sigma > 0:
        perturbation = local_state.normal(loc=0, scale=e, size=weights_size)
        x_t[structure] = x_t[structure] + perturbation[structure]
    # Make sure elements stay within range
    x_t[x_t > 1] = 1
    x_t[x_t < -1] = -1
    # Convert to symmetric matrix
    W_k_it = SymmetricMatrix(x_t)
    # Get initial partition randomly
    initial_partition = GetInitialPartition(n, k, initial_partition_type)
    # Get initial cost value
    initial_z = CutCost(W_k_it, initial_partition, n, k)
    # Get next local maximum
    smoothed_complexity = -1*(SmoothedComplexity(x_t, n, k, initial_partition_type, 0, iters_for_perturbation, heuristic, sigma))
    # Return results as a dictionary key=f, value=x
    results = np.append(x_t, smoothed_complexity)
    output.put(results)
    return x_t, smoothed_complexity

def RunRandomSmoothedComplexity(n, min_weight, max_weight, dropout, initial_partition_type, k, tries, n_perturbations, heuristic, sigma, output):
    weights_size = int(n*(n-1)/2)
    results = []
    for t in range(int(tries)):
        current_seed = np.random.seed(int(time.time()))
        x = InitializeFlatGraph(weights_size, min_weight, max_weight, dropout, seed = current_seed)
        # Keep structure
        structure = np.nonzero(x)
        # Weights size is the length of x
        weights_size = len(x)    
        # Convert to symmetric matrix
        W_k_it = SymmetricMatrix(x)
        # Get initial partition randomly
        initial_partition = GetInitialPartition(n, k, initial_partition_type)
        # Get initial cost value
        initial_z = CutCost(W_k_it, initial_partition, n, k)
        # Get next local maximum
        smoothed_complexity = -1*(SmoothedComplexity(W_k_it, n, k, initial_partition_type, 0, n_perturbations, heuristic, sigma))
        results.append(smoothed_complexity)
    max_result = np.max(results)
    # Return results as a dictionary key=f, value=x
    results = np.append(x, max_result)
    output.put(results)
    return x, smoothed_complexity

class RandomDisplacementBounds(object):
    """random displacement with bounds:  see: https://stackoverflow.com/a/21967888/2320035
        Modified! (dropped acceptance-rejection sampling for a more specialized approach)
    """
    def __init__(self, xmin, xmax, stepsize=0.5):
        self.xmin = xmin
        self.xmax = xmax
        self.stepsize = stepsize

    def __call__(self, x):
        """take a random step but ensure the new position is within the bounds """
        min_step = np.maximum(self.xmin - x, -self.stepsize)
        max_step = np.minimum(self.xmax - x, self.stepsize)

        random_step = np.random.uniform(low=min_step, high=max_step, size=x.shape)
        xnew = x + random_step

        return xnew

## Define grid parameters
# Nodes
min_nodes = 10
max_nodes = 10
num_nodes = 1

# Dropout assumed to be 0
dropout = 0

# Min/max weights
min_weight = -1
max_weight = 1

# K-cuts
ks=[2]

# Number of instances for each combination
tries=1000

# Perturbations
n_perturbations=20

#sigma
sigma = 0.01

# Method
heuristic = "GBF"

initial_partition_type = "0"

nodes, sc = RandomGridSmoothedComplexityParallelK(min_nodes, max_nodes, num_nodes, initial_partition_type, ks, dropout, min_weight, max_weight, n_perturbations, tries, heuristic, sigma)

