# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iIPYAPZ8ED6wNxV0zhI-VvzSH-6AAfi6
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import os
import time
from scipy import stats
import pandas as pd

#%matplotlib notebook
# %matplotlib inline

# Max-cut functions
def InitializeGraph(n, min_weight, max_weight, dropout, seed = -1, graph_type="complete", intercalation=None, modulo=None):
    # Randomness
    if graph_type == "complete":
        if seed == -1:
            W = np.random.uniform(low=min_weight, high=max_weight, size=(n,n))
        else:
            local_state = np.random.RandomState(seed)
            W = local_state.uniform(low=min_weight, high=max_weight, size=(n,n))
        # Dropout with connected
        if dropout>0:
            connected = False
            while not connected:
                if seed == -1:
                    D = np.random.choice([0,1], size=(n,n), replace=True, p=[dropout, 1-dropout])
                else:
                    D = local_state.choice([0,1], size=(n,n), replace=True, p=[dropout, 1-dropout])
                W_dropped = np.multiply(W,D)
                
                # Check for connectivity
                i_lower = np.tril_indices(n, -1)
                W_dropped[i_lower] = W_dropped.T[i_lower]
                # Make sure diagonal is empty
                np.fill_diagonal(W_dropped, 0)
                W_sym = nx.from_numpy_matrix(W_dropped)
                if nx.is_connected(W_sym):
                    connected = True
                    W = np.copy(W_dropped)     
    elif graph_type == "cycle":
        if seed == -1:
            weights = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            weights = local_state.uniform(low=min_weight, high=max_weight, size=(n,n))
        W = np.zeros((n,n))
        i = 0
        j = 1
        for k in range(n-1):
            W[i,j] = weights[k]
            i+=1
            j+=1
        # Add last weight
        W[0, n-1] = weights[n-1]                
    elif graph_type == "regular_log(n)":
        degseq = np.repeat(np.floor(np.log(n)), n)
        adjMatr = AdjacencyMatrixDegree(degseq)
        if seed == -1:
            W = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            W = local_state.uniform(low=min_weight, high=max_weight, size=n)
        W = np.multiply(W,adjMatr)
    elif graph_type == "intercalate":
        if seed == -1:
            weights = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            weights = local_state.uniform(low=min_weight, high=max_weight, size=(n,n))
        adjMatr = np.zeros((n,n))
        # Fill diagonals above the main diagonal
        for k in range(intercalation):
            for i in range(n-k-1):
                adjMatr[i, i+k+1] = 1
        # Fill upper right corner
        for j in range(intercalation):
            for i in range(j+1):
                adjMatr[j-i, n-i-1]=1
        W = np.multiply(weights, adjMatr)
    elif graph_type == "modulo":
        if seed == -1:
            weights = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            weights = local_state.uniform(low=min_weight, high=max_weight, size=(n,n))
        adjMatr = np.zeros((n,n))
        # Create circle
        for i in range(n-1):
            adjMatr[i, i+1] = 1
        # Fill upper right corner
        adjMatr[0, n-1]=1
        # Add other connections
        # Fill diagonals above the main diagonal
        for i in range(n-modulo):
            adjMatr[i, i+modulo] = 1
        # Fill upper right corner
        for i in range(modulo):
            adjMatr[modulo-1-i, n-i-1]=1
        W = np.multiply(weights, adjMatr)
    
    # Copy upper diagonal to lower diagonal (only the upper perturbation counts)
    i_lower = np.tril_indices(n, -1)
    W[i_lower] = W.T[i_lower]
    # Make sure diagonal is empty
    np.fill_diagonal(W, 0)
    
    return W

def InitializeFlatGraph(n, min_weight, max_weight, dropout = 0, seed = -1, graph_type = "complete", intercalation = None, modulo = None):
    if graph_type == "complete":
        if seed == -1:
            W = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            W = local_state.uniform(low=min_weight, high=max_weight, size=n)
        if dropout>0:            
            connected = False
            while connected == False:
                if seed == -1:
                    D = np.random.choice([0,1], size=n, replace=True, p=[dropout, 1-dropout])
                else:
                    D = local_state.choice([0,1], size=n, replace=True, p=[dropout, 1-dropout])
                # Check if the dropped out matrix is connected
                # Make it a symmetric matrix
                D_sym = SymmetricMatrix(D)
                # Check if it resembles a connected graph
                D_sym = nx.from_numpy_matrix(D_sym)
                if nx.is_connected(D_sym):
                    connected = True
            # Once we have a conencted graph, add weights
            W = np.multiply(W,D)       
    elif graph_type == "cycle":
        if seed == -1:
            weights = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            weights = local_state.uniform(low=min_weight, high=max_weight, size=n)
        W = np.zeros((n,n))
        i = 0
        j = 1
        for k in range(n-1):
            W[i,j] = weights[k]
            i+=1
            j+=1
        # Add last weight
        W[0, n-1] = weights[n-1]
        i_lower = np.tril_indices(n, -1)
        W[i_lower] = W.T[i_lower]
        # Make sure diagonal is empty
        np.fill_diagonal(W, 0)
        W = np.copy(W[np.triu_indices_from(W, k = 1)])
    elif graph_type == "regular_log(n)":
        degseq = np.repeat(np.floor(np.log(n)), n)
        adjMatr = AdjacencyMatrixDegree(degseq)
        if seed == -1:
            W = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            W = local_state.uniform(low=min_weight, high=max_weight, size=n)
        W = np.multiply(W,adjMatr)
        W = np.copy(W[np.triu_indices_from(W, k = 1)])
    elif graph_type == "intercalate":
        W = InitializeGraph(n, min_weight, max_weight, dropout, seed, graph_type, intercalation)
        W = np.copy(W[np.triu_indices_from(W, k = 1)])
    elif graph_type == "modulo":
        W = InitializeGraph(n, min_weight, max_weight, dropout, seed, graph_type, None, modulo)
        W = np.copy(W[np.triu_indices_from(W, k = 1)])
    return W

def AdjacencyMatrixDegree(degseq):
    # n is number of vertices  
    n = len(degseq)
    mat = [[0] * n for i in range(n)] 
    for i in range(n): 
        for j in range(i + 1, n): 
            # For each pair of vertex decrement  
            # the degree of both vertex.  
            if (degseq[i] > 0 and degseq[j] > 0): 
                degseq[i] -= 1
                degseq[j] -= 1
                mat[i][j] = 1
                mat[j][i] = 1
    return mat

def GetAllPossibleAdjacencyFlatMatrixes(n):
    import itertools
    flatMatrixes = list(itertools.product([0, 1], repeat=n))
    return flatMatrixes

def GetKRandomPartition(n, k, seed = -1):
    partition = {}
    if seed == -1:
        for i in range(n):
            partition[i]=np.random.randint(low=0, high=k, size=1)[0]
    else:
        local_state = np.random.RandomState(seed)
        for i in range(n):
            partition[i]=local_state.randint(low=0, high=k, size=1)[0]
    return partition

# G stands for special ill-posed matrixes
def GetGWeightMatrix(g):
    if g == 1:
        G = np.array([[0, 7, 0, 0, 0, 0, 0, 0],
             [0, 0, 6, 0, 0, 0, 0, 0],
             [0, 0, 0, 5, 0, 0, 0, 0],
             [0, 0, 0, 0, 1, 3, 0, 0],
             [0, 0, 0, 0, 0, 0, 1, 1],
             [0, 0, 0, 0, 0, 0, 2, 0],
             [0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0]]).astype(float)
    elif g == 2:
        G = np.array([
               [0, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               [0, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 49, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 0, 8, 0, 10, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 8, 0, 8, 5, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 4, 0],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]).astype(float)
    else:
        raise("No")
    n = np.size(G, 1)
    i_lower = np.tril_indices(n, -1)
    G[i_lower] = G.T[i_lower]
    return G

def WeightsStats(W):
    print("Weight statistics\n")
    mean = []
    median = []
    std = []
    min_w = []
    max_w = []
    n = []
    W_without_0_edges = np.copy(W[W!=0])
    mean.append(W_without_0_edges.mean())
    median.append(np.median(W_without_0_edges))
    std.append(W_without_0_edges.std())
    min_w.append(W_without_0_edges.min())
    max_w.append(W_without_0_edges.max())
    s = W_without_0_edges.shape[0]
    s = int((1+np.sqrt(1+4*2*s))/2)+1
    n.append(s)
    print(tabulate(list(zip(*[n, mean, median, std, min_w, max_w])), 
                   headers=["n", 'mean', 'median', 'std', "min_weight", "max_weight"], floatfmt=".3f"))
    return

def Get0Partition(n):
    partition = {}
    for i in range(n):
        partition[i]=0
    return partition

def Get1Partition(n):
    partition = {}
    for i in range(n):
        partition[i]=1
    return partition

def GetStupidPartition(n):
    partition = {}
    for i in range(n):
        partition[i]=-1
    return partition

def GetInitialPartition(n, k, initial_partition_type, seed = -1):
    if initial_partition_type == "random":
        initial_partition = GetKRandomPartition(n, k, seed)
    if initial_partition_type == "0":
        initial_partition = Get0Partition(n)
    if initial_partition_type == "1":
        initial_partition = Get1Partition(n)
    return initial_partition

def CutCost(W, partition, n, k):
    z = 0
    identity = np.identity(k)
    for i in range(n):
        for j in range(i, n):
            new_edge = (1-identity[partition[i], partition[j]])*W[i,j]
            z += new_edge
    return z
        
def CutGainAfterFlip(W, initial_partition, final_partition, n):
    # Vi: initial partition of the vertex v
    # Vj: final partition of the vertex v
    # z: current cut
    weights_to_add = 0
    weights_to_substract = 0
    
    diff_partition = {key: initial_partition[key] - final_partition.get(key, 0) for key in initial_partition}
    
    # v: vertex that flipped
    v = [key for key, val in diff_partition.items() if val!=0][0]
    v_old_partition = initial_partition[v]
    v_new_partition = final_partition[v]
    
    # Vertices belonging to the old group and the new group
    vertices_in_old_partition = [key  for (key, value) in initial_partition.items() if value == v_old_partition]
    vertices_in_new_partition = [key  for (key, value) in initial_partition.items() if value == v_new_partition]

    for vi in vertices_in_old_partition:
        if vi == v:
            continue
        else:
            weights_to_add += W[v, vi]
    for vj in vertices_in_new_partition:
        weights_to_substract += W[v, vj]
    z = weights_to_add - weights_to_substract
    return z
            
def SolveMaxCut(W, n, k, z, initial_partition, heuristic, seed = -1):
    tol = 1e-8
    maxIter = 100000
    it = 0
    
    zt1 = -10000
    zt2 = CutCost(W, initial_partition, n, k)
    
    # In case the cut is already optimal
    new_z = zt2
    
    # Store old and new partition
    old_partition = GetStupidPartition(n)
    new_partition = initial_partition
    
    start = time.time()
    if heuristic == "GBF":
        # Greedy Best Flip
        while(old_partition!=new_partition and it<maxIter):
            old_partition = new_partition
            new_partition, new_z = GreedyBestFlip(W, new_partition, n, k, zt2)
            zt1 = zt2
            zt2 = new_z
            if old_partition!=new_partition:
                it = it + 1
    elif heuristic == "RPF":
        # Random Positive Flip
         while(old_partition!=new_partition and it<maxIter):
            old_partition = new_partition
            new_partition, new_z = RandomPositiveFlip(W, new_partition, n, k, zt2, seed)
            zt1 = zt2
            zt2 = new_z
            if old_partition!=new_partition:
                it = it + 1
    elif heuristic == "WF":
        # Worst Flip
         while(old_partition!=new_partition and it<maxIter):
            old_partition = new_partition
            new_partition, new_z = WorstFlip(W, new_partition, n, k, zt2)
            zt1 = zt2
            zt2 = new_z
            if old_partition!=new_partition:
                it = it + 1

    elif heuristic == "FNF":
        # First Next Flip
        # Only for testing purposes
        iters_fnf = 3
        for i in range(iters_fnf):
            partition, new_z = FirstNextFlip(W, partition, n, k, zt2)
            zt1 = zt2
            zt2 = new_z
            if old_partition!=new_partition:
                it = it + 1
            
    end = time.time()
    elapsed_time = np.round(end - start, 4)
    # The last step would return the same partition, so it-1
    return new_partition, new_z, elapsed_time, it

def SolveGlobalMaxCut(W, n, k):
    # Get all possible partitions
    import itertools
    k_vector = range(k)
    allPartitions = list(itertools.product(k_vector, repeat=n))
    n_partitions = len(allPartitions)
    max_cut = -100000
    best_partition = allPartitions[0]
    start = time.time()
    for partition in allPartitions:
        c = CutCost(W, partition, n, k)
        if c > max_cut:
            max_cut = c
            best_partition = partition
    end = time.time()
    elapsed_time = np.round(end - start, 4)
    # Create the partition dictionary
    best_partition = list(best_partition)
    keys = range(0, n)
    best_partition_d = {k:v for k,v in zip(keys, best_partition)}
    # The last step would return the same partition, so it-1
    return best_partition_d, max_cut, elapsed_time

def SymmetricMatrix(W):
    m = len(W)
    n = int((1+np.sqrt(1+4*2*m))/2)
    A = np.zeros(shape=(n, n))
    # k runs along W
    k = 0
    # Fill upper diagonal
    for i in range(n-1):
        for j in range(i+1, n):
            A[i,j]=W[k]
            k+=1
    # Fill lower diagonal
    i_lower = np.tril_indices(n, -1)
    A[i_lower] = A.T[i_lower]
    return A

def ScaleMatrix(W, a, b):
    non_zeros = np.nonzero(W)
    min_W = W[non_zeros].min()
    max_W = W.max()
    # Keep structure
    W_scaled = np.copy(W.astype(float))
    W_scaled[non_zeros] = (b-a)*((W[non_zeros]-min_W)/(max_W-min_W))+a
    return W_scaled

def NumpyToCsv(array, filename):
    np.savetxt(str(filename) + ".csv", array, delimiter=";")

def DeleteTempResults(filename):
    if os.path.exists(filename):
        os.remove(filename)
    return

def CsvToNumpy(filename):
    return np.genfromtxt(str(filename) + ".csv", delimiter=";")

def KolmogorovSmirnovTest(x1, x2, alpha):
    st, p = stats.ks_2samp(x1, x2)
    n1 = len(x1)
    n2 = len(x2)
    size_term = np.sqrt((n1+n2)/(n1*n2))
    if alpha == 0.05:
        d = size_term*1.36
    elif alpha == 0.025:
        d = size_term*1.48
    else:
        return
    if st < d:
        print("Both distributions are the same with probability", 100*(1-alpha), "%")
        print(st, "<", d)
    if st > d:
        print("Reject null hypothesis: different distributions with probability", 100*(1-alpha), "%")
        print(st, ">", d)
    return
    
# Meshgrid - No need to parallelize
def MeshGrid3dMaxCut(k, min_weight, max_weight, initial_partition_type, n_points_ax, n_iters, complexity, heuristic, sigma):
    weights_size = 3
    x = np.zeros(shape=(np.power(n_points_ax+1, 3), weights_size))
    f = np.zeros(np.power(n_points_ax+1, 3))
    # p runs over all points
    p = 0
    for i in range(n_points_ax+1):
        x_val = min_weight + (max_weight-min_weight)*i/n_points_ax
        for j in range(n_points_ax+1):
            y_val = min_weight + (max_weight-min_weight)*j/n_points_ax
            for l in range(n_points_ax+1):
                z_val = min_weight + (max_weight-min_weight)*l/n_points_ax
                w_flat = [x_val, y_val, z_val]
                w = SymmetricMatrix(w_flat)
                steps = np.zeros(n_iters)
                for it in range(n_iters):
                    initial_partition = GetInitialPartition(3, k, initial_partition_type)
                    initial_z = CutCost(w, initial_partition, 3, k)
                    if complexity == "average":
                        _p, _z, _t, steps[it] = SolveMaxCut(w, 3, k, initial_z, initial_partition, heuristic)
                    elif complexity == "smoothed":
                        # n_iters is also used for n_perturbations
                        steps[it] = SmoothedComplexity(w_flat, 3, k, initial_partition_type, 0, n_iters, heuristic, sigma)
                f[p] = np.mean(steps)
                x[p] = [x_val, y_val, z_val]
                p+=1
    return x, f

def PlotMeshGrid3d(x, f, title, colorsMap='jet'):
    cm = plt.get_cmap(colorsMap)
    cNorm = matcolors.Normalize(vmin=min(f), vmax=max(f))
    scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cm)
    fig = plt.figure()
    ax = Axes3D(fig)
    ax.scatter(x[:,0], x[:,1], x[:,2], c=scalarMap.to_rgba(f))
    plt.xticks(np.arange(min(x[:,0]), max(x[:,0])+1, 0.5))
    plt.yticks(np.arange(min(x[:,1]), max(x[:,1])+1, 0.5))
    scalarMap.set_array(f)
    fig.colorbar(scalarMap)
    if title:
        plt.title(title)
    plt.show()

def SolveGlobalAndLocalMaxCut(nodes, iters, min_weight, max_weight, heuristic, k, initial_partition_type, dropout, storeCSV=False):
    global_results = np.zeros((len(nodes), 1))
    local_results = np.zeros((len(nodes), 1))
    global_sd = np.zeros((len(nodes), 1))
    local_sd = np.zeros((len(nodes), 1))
    t = 0
    # Save nodes
    if storeCSV:
        NumpyToCsv(nodes, "nodes")
    for n in nodes:
        initial_partition = GetInitialPartition(n, k, initial_partition_type)
        global_result_iters = []
        local_result_iters = []
        for it in range(iters):
            # Global cut
            W = InitializeGraph(n, min_weight, max_weight, dropout)
            _p, global_result_it, _e = SolveGlobalMaxCut(W, n, k)
            global_result_iters.append(global_result_it)
            # Local max-cut
            initial_cut = CutCost(W, initial_partition, n, k)
            _p, local_result_it, _t, _it = SolveMaxCut(W, n, k, initial_cut, initial_partition, heuristic)
            local_result_iters.append(local_result_it)
        # Get the average values and append them
        global_results[t]=np.mean(np.array(global_result_iters))
        local_results[t] = np.mean(np.array(local_result_iters))
        # Get the sd and append them
        global_sd[t] = np.std(np.array(global_result_iters))
        local_sd[t] = np.std(np.array(local_result_iters))
        # Store
        if storeCSV:
            DeleteTempResults("global_results.csv")
            NumpyToCsv(global_results, "global_results")
            DeleteTempResults("local_results.csv")
            NumpyToCsv(local_results, "local_results")
            DeleteTempResults("global_results_sd.csv")
            NumpyToCsv(global_results_sd, "global_results_sd")
            DeleteTempResults("local_results_sd.csv")
            NumpyToCsv(local_results_sd, "local_results_sd")
        t+= 1    
    # Convert to proper format
    global_results = np.reshape(global_results, (len(nodes),1))
    local_results = np.reshape(local_results, (len(nodes),1))
    global_sd = np.reshape(global_sd, (len(nodes),1))
    local_sd = np.reshape(local_sd, (len(nodes),1))
    return global_results, global_sd, local_results, local_sd

# Plot results
def GetColorsVector(n):
    all_colors = ["blue", "red", "green", "black", "purple", "grey"]
    return all_colors[:n]

def PlotGraph(W, partition = None, ax=None):
    G = nx.from_numpy_matrix(np.round(W, 3))
    # Relabel nodes
    #mapping = {0: '1', 1: '2', 2: '3'}
    #G = nx.relabel_nodes(G, mapping)
    pos = nx.circular_layout(G)
    
    # Create edge labels
    labels = nx.get_edge_attributes(G, 'weight')
    edges, weights = zip(*nx.get_edge_attributes(G,'weight').items())
    
    # Draw the graph according to node positions
    if partition is not None:
        colors = np.fromiter(partition.values(), dtype=int)
        if ax is None:
            nx.draw(G, pos, with_labels=True, node_color=colors, edgelist=edges, edge_color=weights, width=5.0, edge_cmap=plt.cm.Blues)
        else:
            nx.draw(G, pos, with_labels=True, node_color=colors, edgelist=edges, edge_color=weights, width=5.0, edge_cmap=plt.cm.Blues, ax=ax)
    else:
        if ax is None:
            nx.draw(G, pos, with_labels=True, edgelist=edges, edge_color=weights, width=5.0, edge_cmap=plt.cm.Blues)
        else:
            nx.draw(G, pos, with_labels=True, edgelist=edges, edge_color=weights, width=5.0, edge_cmap=plt.cm.Blues, ax=ax)
    # Draw edge labels according to node positions
    #nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)
    if ax is None:
        plt.show()    
    return

def PlotWeightsHistogram(W):
    # Start plot
    plt.figure(1)
    plt.subplot(211)
    # Plot histogram of weights
    W_without_0_edges = np.copy(W[W!=0])
    plt.hist(W_without_0_edges, bins=20, color='blue')
    plt.title('Weight distribution', fontsize=8)
    # Do KSTest with uniform
    n = len(W)
    uniform_sample = np.random.uniform(low=-1, high=1, size=n)
    KolmogorovSmirnovTest(W, uniform_sample, alpha=0.025)
    # Histogram of the sum of the edges from each vertex
    sum_of_edges = np.sum(SymmetricMatrix(W), axis = 0)
    plt.subplot(212)
    plt.hist(sum_of_edges, bins=10)
    plt.title('Sum of edges', fontsize=8)
    plt.tight_layout()
    plt.show()
    # Do KSTest with artificial sample (assume independency)
    #artificial_sample = np.zeros(n)
    #for i in range(n):
    #    artificial_sample[i] = np.sum(np.random.uniform(low=-1, high=1, size=n-1))
    #KolmogorovSmirnovTest(sum_of_edges, artificial_sample, 0.05)
    return

def PlotRegressionResults(nodes, steps_mean, steps_sd, with_errorbars, r_var, r_var_label, method):
    
    # Convert X axis according to method
    x = nodes
    if method=="polynomial":
        x = np.log(x.reshape(-1, 1))
    if method == "quasipolynomial":
        x = np.log(x.reshape(-1, 1))*x.reshape(-1, 1)

    cmap = GetColorsVector(len(r_var))
    
    # t runs over all columns of steps_mean
    t=0
    slopes=[]
    intercepts=[]
    r_values=[]
    p_values=[]
    std_errs=[]

    # Create a new plot
    plt.figure(np.random.randint(501, 1000))
    
    for r_v in r_var:
        # Convert Y axis according to method
        y = steps_mean[:,t]
        if (method=="polynomial") or (method=="exponential") or (method=="quasipolynomial"):
            y_log = np.log(y)
        plt.scatter(x, y_log, c=cmap[t])
        if with_errorbars:
            y_sd = steps_sd[:, t]/y
            plt.errorbar(x,y_log,yerr=y_sd, linestyle="None", c=cmap[t])

        # Linear regression
        slope, intercept, r_value, p_value, std_err = stats.mstats.linregress(x,y_log)

        # Store results
        slopes.append(slope)
        intercepts.append(intercept)
        r_values.append(r_value)
        p_values.append(p_value)
        std_errs.append(std_err)

        # Linear regression prediction
        y_reg = intercept + slope*x
        plt.plot(x, y_reg, label=str(r_var_label) + '=' + str(r_v), c=cmap[t])
        t+=1

    if method=="polynomial":
        plt.title('Polynomial behavior')     
        plt.xlabel('ln(Nodes)') 
        plt.ylabel('ln(Steps)')
    
    if method=="exponential":
        plt.title('Exponential behavior')     
        plt.xlabel('Nodes') 
        plt.ylabel('ln(Steps)')
        
    if method=="quasipolynomial":
        plt.title('Quasiplynomial behavior')     
        plt.xlabel('ln(Nodes)*Nodes') 
        plt.ylabel('ln(Steps)')
    
    plt.legend()        
    plt.show()

    # Print table
    print("Regression results:\n")
    print(tabulate(list(zip(*[r_var, slopes, intercepts, r_values])), headers=[r_var_label, 'slope', 'intercept', 'r_value'], floatfmt=".3f"))
    
    return slopes, intercepts
    
def PlotResults(nodes, steps_mean, steps_sd, with_errorbars, r_var, r_var_label, xlabel = "Nodes", ylabel = "Steps"):
    #r_var is running variable
    
    # Show results for each r_var
    x = nodes.reshape(-1, 1)

    # Legend
    r_var_labels=[]

    # t runs over all columns of steps_mean
    t=0

    cmap = GetColorsVector(len(r_var))
    
    # Create a new plot
    plt.figure(np.random.randint(0, 500))
    
    for r_v in r_var:
        run_steps = steps_mean[:,t]
        plt.scatter(x, run_steps, c=cmap[t])
        if with_errorbars:
            run_steps_sd = steps_sd[:, t]
            plt.errorbar(x,run_steps,yerr=run_steps_sd, linestyle="None", c=cmap[t])
        r_var_labels.append(str(r_var_label) + "=" + str(r_v))
        t+=1

    plt.title(str(xlabel) + " vs. " + str(ylabel)) 
    plt.xlabel(xlabel) 
    plt.ylabel(ylabel)
    plt.legend(r_var_labels)
    plt.show()
    
def PlotResultsSmoothed(nodes, steps_mean, steps_sd, with_errorbars, r_var, r_var_label, sigma, withUpperBound):
    #r_var is running variable
    
    # Show results for each r_var
    x = nodes.reshape(-1, 1)

    # Legend
    r_var_labels=[]

    # t runs over all columns of steps_mean
    t=0
    
    # Create a new plot
    plt.figure(np.random.randint(1001, 1500))

    for r_v in r_var:
        run_steps = steps_mean[:,t]
        plt.scatter(x, run_steps)
        if with_errorbars:
            run_steps_sd = steps_sd[:, t]
            plt.errorbar(x,run_steps,yerr=run_steps_sd, linestyle="None")
        r_var_labels.append(str(r_var_label) + "=" + str(r_v))
        t+=1
    # Assume gaussian
    if withUpperBound:
        phi = 1/np.sqrt(2*np.pi*sigma*sigma)
        upper_bound = phi*np.power(nodes, 7.83)
        plt.plot(nodes, upper_bound, color='red', linewidth=1.0, linestyle='--')
    plt.title('Steps vs. Nodes') 
    plt.xlabel('Nodes') 
    plt.ylabel('Steps')
    plt.legend(r_var_labels)
    plt.show()
    return

def PlotTwoRegressionResults(nodes1, steps_mean1, steps_sd1, nodes2, steps_mean2, steps_sd2, with_errorbars, r_var, r_var_label, method, labels):
    
    # Create a new plot
    plt.figure(np.random.randint(501, 1000))

    # t runs over all columns of steps_mean
    t=0
    slopes1=[]
    intercepts1=[]
    r_values1=[]
    p_values1=[]
    std_errs1=[]    
    # Convert X axis according to method
    x1 = nodes1
    if method=="polynomial":
        x1 = np.log(x1.reshape(-1, 1))
    if method == "quasipolynomial":
        x1 = np.log(x1.reshape(-1, 1))*x1.reshape(-1, 1)
    
    # For the averae case, we have to reduce the number of vertices so that the have similar lengths
    last_index = np.argmax(nodes1>np.max(nodes2))
    if last_index == 0:
        last_index = len(x1-1)
    x1 = x1[:last_index]
    steps_mean1 = steps_mean1[:last_index]
    steps_sd1 = steps_sd1[:last_index]
    
    for r_v in r_var:
        # Convert Y axis according to method
        y1 = steps_mean1[:,t]
        if (method=="polynomial") or (method=="exponential") or (method=="quasipolynomial"):
            y1 = np.log(y1)
        plt.scatter(x1, y1, c = "darkred", alpha=0.5)
        if with_errorbars:
            if len(steps_sd1)>0:
                y_sd1 = steps_sd1[:, t]
                plt.errorbar(x1,y1,yerr=y_sd1, linestyle="None", c = "darkred", alpha=0.5)

        # Linear regression
        slope1, intercept1, r_value1, p_value1, std_err1 = stats.mstats.linregress(x1,y1)

        # Store results
        slopes1.append(slope1)
        intercepts1.append(intercept1)
        r_values1.append(r_value1)
        p_values1.append(p_value1)
        std_errs1.append(std_err1)

        # Linear regression prediction
        y_reg1 = intercept1 + slope1*x1
        plt.plot(x1, y_reg1, label=labels[0], c = "red")
        t+=1
        
    # t runs over all columns of steps_mean
    t=0
    slopes2=[]
    intercepts2=[]
    r_values2=[]
    p_values2=[]
    std_errs2=[]   
    
    # X2 is smoothed, therefore no error bars
    # Convert X axis according to method
    x2 = nodes2
    if method=="polynomial":
        x2 = np.log(x2.reshape(-1, 1))
    if method == "quasipolynomial":
        x2 = np.log(x2.reshape(-1, 1))*x2.reshape(-1, 1)
    
    for r_v in r_var:
        # Convert Y axis according to method
        y2 = steps_mean2[:,t]
        if (method=="polynomial") or (method=="exponential") or (method=="quasipolynomial"):
            y2 = np.log(y2)
        plt.scatter(x2, y2, c = "darkblue", alpha=0.5)
        if with_errorbars:
            if len(steps_sd2)>0:
                y_sd2 = steps_sd2[:, t]
                plt.errorbar(x2,y2,yerr=y_sd2, linestyle="None", c = "darkblue", alpha=0.5)

        # Linear regression
        slope2, intercept2, r_value2, p_value2, std_err2 = stats.mstats.linregress(x2,y2)

        # Store results
        slopes2.append(slope2)
        intercepts2.append(intercept2)
        r_values2.append(r_value2)
        p_values2.append(p_value2)
        std_errs2.append(std_err2)

        # Linear regression prediction
        y_reg2 = intercept2 + slope2*x2
        plt.plot(x2, y_reg2, label=labels[1], c = "blue")
        t+=1

    if method=="polynomial":
        plt.title('Polynomial behavior')     
        plt.xlabel('ln(Nodes)') 
        plt.ylabel('ln(Steps)')
    
    if method=="exponential":
        plt.title('Exponential behavior')     
        plt.xlabel('Nodes') 
        plt.ylabel('ln(Steps)')
        
    if method=="quasipolynomial":
        plt.title('Quasiplynomial behavior')     
        plt.xlabel('ln(Nodes)*Nodes') 
        plt.ylabel('ln(Steps)')
    
    plt.legend(loc="upper left")        
    plt.show()

    # Print table
    print("Regression results:\n")
    print(labels[0])
    print(tabulate(list(zip(*[r_var, slopes1, intercepts1, r_values1])), headers=[r_var_label, 'slope', 'intercept', 'r_value'], floatfmt=".3f"))
    print(labels[1])
    print(tabulate(list(zip(*[r_var, slopes2, intercepts2, r_values2])), headers=[r_var_label, 'slope', 'intercept', 'r_value'], floatfmt=".3f"))
    
    return

def PlotThreeRegressionResults(nodes1, steps_mean1, steps_sd1, nodes2, steps_mean2, steps_sd2, nodes3, steps_mean3, steps_sd3,
                               with_errorbars, r_var, r_var_label, method, labels):
    
    # Create a new plot
    plt.figure(np.random.randint(501, 1000))

    # t runs over all columns of steps_mean
    t=0
    slopes1=[]
    intercepts1=[]
    r_values1=[]
    p_values1=[]
    std_errs1=[]    
    # Convert X axis according to method
    x1 = nodes1
    if method=="polynomial":
        x1 = np.log(x1.reshape(-1, 1))
    if method == "quasipolynomial":
        x1 = np.log(x1.reshape(-1, 1))*x1.reshape(-1, 1)
    
    # For the averae case, we have to reduce the number of vertices so that the have similar lengths
    last_index = np.argmax(nodes1>np.max(nodes2))
    if last_index == 0:
        last_index = len(x1-1)
    x1 = x1[:last_index]
    steps_mean1 = steps_mean1[:last_index]
    steps_sd1 = steps_sd1[:last_index]
    
    for r_v in r_var:
        # Convert Y axis according to method
        y1 = steps_mean1[:,t]
        if (method=="polynomial") or (method=="exponential") or (method=="quasipolynomial"):
            y1 = np.log(y1)
        plt.scatter(x1, y1, c = "darkred", alpha=0.5)
        if with_errorbars:
            if len(steps_sd1)>0:
                y_sd1 = steps_sd1[:, t]
                plt.errorbar(x1,y1,yerr=y_sd1, linestyle="None", c = "darkred", alpha=0.5)

        # Linear regression
        slope1, intercept1, r_value1, p_value1, std_err1 = stats.mstats.linregress(x1,y1)

        # Store results
        slopes1.append(slope1)
        intercepts1.append(intercept1)
        r_values1.append(r_value1)
        p_values1.append(p_value1)
        std_errs1.append(std_err1)

        # Linear regression prediction
        y_reg1 = intercept1 + slope1*x1
        plt.plot(x1, y_reg1, label=labels[0], c = "red")
        t+=1
        
    # t runs over all columns of steps_mean
    t=0
    slopes2=[]
    intercepts2=[]
    r_values2=[]
    p_values2=[]
    std_errs2=[]   
    
    # X2 is smoothed, therefore no error bars
    # Convert X axis according to method
    x2 = nodes2
    if method=="polynomial":
        x2 = np.log(x2.reshape(-1, 1))
    if method == "quasipolynomial":
        x2 = np.log(x2.reshape(-1, 1))*x2.reshape(-1, 1)
    
    for r_v in r_var:
        # Convert Y axis according to method
        y2 = steps_mean2[:,t]
        if (method=="polynomial") or (method=="exponential") or (method=="quasipolynomial"):
            y2 = np.log(y2)
        plt.scatter(x2, y2, c = "darkblue", alpha=0.5)
        if with_errorbars:
            if len(steps_sd2)>0:
                y_sd2 = steps_sd2[:, t]
                plt.errorbar(x2,y2,yerr=y_sd2, linestyle="None", c = "darkblue", alpha=0.5)

        # Linear regression
        slope2, intercept2, r_value2, p_value2, std_err2 = stats.mstats.linregress(x2,y2)

        # Store results
        slopes2.append(slope2)
        intercepts2.append(intercept2)
        r_values2.append(r_value2)
        p_values2.append(p_value2)
        std_errs2.append(std_err2)

        # Linear regression prediction
        y_reg2 = intercept2 + slope2*x2
        plt.plot(x2, y_reg2, label=labels[1], c = "blue")
        t+=1
        
    # t runs over all columns of steps_mean
    t=0
    slopes3=[]
    intercepts3=[]
    r_values3=[]
    p_values3=[]
    std_errs3=[]   
    
    # Convert X axis according to method
    x3 = nodes3
    if method=="polynomial":
        x3 = np.log(x3.reshape(-1, 1))
    if method == "quasipolynomial":
        x3 = np.log(x3.reshape(-1, 1))*x3.reshape(-1, 1)
    
    for r_v in r_var:
        # Convert Y axis according to method
        y3 = steps_mean3[:,t]
        if (method=="polynomial") or (method=="exponential") or (method=="quasipolynomial"):
            y3 = np.log(y3)
        plt.scatter(x3, y3, c = "darkgreen", alpha=0.5)
        if with_errorbars:
            if len(steps_sd3)>0:
                y_sd3 = steps_sd3[:, t]
                plt.errorbar(x3,y3,yerr=y_sd3, linestyle="None", c = "darkgreen", alpha=0.5)

        # Linear regression
        slope3, intercept3, r_value3, p_value3, std_err3 = stats.mstats.linregress(x3,y3)

        # Store results
        slopes3.append(slope3)
        intercepts3.append(intercept3)
        r_values3.append(r_value3)
        p_values3.append(p_value3)
        std_errs3.append(std_err3)

        # Linear regression prediction
        y_reg3 = intercept3 + slope3*x3
        plt.plot(x3, y_reg3, label=labels[2], c = "green")
        t+=1

    if method=="polynomial":
        plt.title('Polynomial behavior')     
        plt.xlabel('ln(Nodes)') 
        plt.ylabel('ln(Steps)')
    
    if method=="exponential":
        plt.title('Exponential behavior')     
        plt.xlabel('Nodes') 
        plt.ylabel('ln(Steps)')
        
    if method=="quasipolynomial":
        plt.title('Quasiplynomial behavior')     
        plt.xlabel('ln(Nodes)*Nodes') 
        plt.ylabel('ln(Steps)')
    
    plt.legend(loc="upper left")        
    plt.show()

    # Print table
    print("Regression results:\n")
    print(labels[0])
    print(tabulate(list(zip(*[r_var, slopes1, intercepts1, r_values1])), headers=[r_var_label, 'slope', 'intercept', 'r_value'], floatfmt=".3f"))
    print(labels[1])
    print(tabulate(list(zip(*[r_var, slopes2, intercepts2, r_values2])), headers=[r_var_label, 'slope', 'intercept', 'r_value'], floatfmt=".3f"))
    print(labels[2])
    print(tabulate(list(zip(*[r_var, slopes3, intercepts3, r_values3])), headers=[r_var_label, 'slope', 'intercept', 'r_value'], floatfmt=".3f"))
    
    return

def MultilinearRegression(X, Y, method):
    import statsmodels.api as sm
    from sklearn import linear_model
    if method == "log-log":
        X = np.log(X)
        Y = np.log(Y)
    X = sm.add_constant(X) # adding a constant
    model = sm.OLS(Y, X).fit()
    #predictions = model.predict(X) 
    print_model = model.summary()
    print(print_model)
    return model.params, model.rsquared

def GreedyBestFlip(W, partition, n, k, z):
    cut_costs = []
    partitions_flip = []
    # Permutate vertices so that we do not always start by the same one
    order = np.random.permutation(range(n))
    for i in order:
        for j in range(1, k):            
            new_partition = partition.copy()
            new_partition[i] = (new_partition[i]+j)%k
            partitions_flip.append(new_partition)
            cut_costs.append(CutGainAfterFlip(W, partition, new_partition, n))
      
    # Convert the list to a numpy array
    cut_costs = np.asarray(cut_costs)
    if np.any(cut_costs[cut_costs>0]):
        # New best partition was found
        best_index = np.argmax(cut_costs)
        best_partition = partitions_flip[best_index]
        new_cut_cost = z + cut_costs[best_index]
        return best_partition, new_cut_cost
    else:
        # We are in a local optimum
        return partition, z
    
def RandomPositiveFlip(W, partition, n, k, z, seed = -1):
    # Permutate vertices so that we do not always start by the same one
    # Randomness
    if seed == -1:
        order = np.random.permutation(range(n))
    else:
        local_state = np.random.RandomState(seed)
        order = local_state.permutation(range(n))
    for i in order:
        for j in range(1, k):            
            new_partition = partition.copy()
            new_partition[i] = (new_partition[i]+j)%k
            new_z = CutGainAfterFlip(W, partition, new_partition, n)
            if new_z > 0:
                return new_partition, z + new_z
      
    return partition, z

def WorstFlip(W, partition, n, k, z):
    cut_costs = []
    partitions_flip = []
    # Permutate vertices so that we do not always start by the same one
    order = np.random.permutation(range(n))
    for i in order:
        for j in range(1, k):            
            new_partition = partition.copy()
            new_partition[i] = (new_partition[i]+j)%k
            partitions_flip.append(new_partition)
            cut_costs.append(CutGainAfterFlip(W, partition, new_partition, n))
      
    # Convert the list to a numpy array
    cut_costs = np.asarray(cut_costs)
    
    if np.any(cut_costs[cut_costs>0]):
        # A better partition was found
        min_val = min(c for c in cut_costs if c > 0)
        worst_index = np.argwhere(cut_costs==min_val)[0][0]
        worst_partition = partitions_flip[worst_index]
        new_cut_cost = z + cut_costs[worst_index]
        return worst_partition, new_cut_cost
    else:
        # We are in a local optimum
        return partition, z

nodes = np.linspace(3, 5, num=2)
# Remove decimal part
nodes = np.floor(nodes)
# Convert to integer
nodes = nodes.astype(int)
iters = 20
min_weight = -1
max_weight = 1
heuristic = "GBF"
k = 2
initial_partition = "0"
dropout = 0

global_results, global_sd, local_results, local_sd = SolveGlobalAndLocalMaxCut(nodes, iters, min_weight, max_weight, heuristic, k, initial_partition, dropout)

