###TESTER
# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BHHsLNfrq_nSoSu570m3osZ3Njbdncc9
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import os
import time
from scipy import stats
import pandas as pd
import gurobipy as gp
from gurobipy import GRB

#%matplotlib notebook
# %matplotlib inline

# Max-cut functions
def InitializeGraph(n, min_weight, max_weight, dropout, seed = -1, graph_type="complete", intercalation=None, modulo=None):
    # Randomness
    if graph_type == "complete":
        if seed == -1:
            W = np.random.uniform(low=min_weight, high=max_weight, size=(n,n))
        else:
            local_state = np.random.RandomState(seed)
            W = local_state.uniform(low=min_weight, high=max_weight, size=(n,n))
        # Dropout with connected
        if dropout>0:
            connected = False
            while not connected:
                if seed == -1:
                    D = np.random.choice([0,1], size=(n,n), replace=True, p=[dropout, 1-dropout])
                else:
                    D = local_state.choice([0,1], size=(n,n), replace=True, p=[dropout, 1-dropout])
                W_dropped = np.multiply(W,D)
                
                # Check for connectivity
                i_lower = np.tril_indices(n, -1)
                W_dropped[i_lower] = W_dropped.T[i_lower]
                # Make sure diagonal is empty
                np.fill_diagonal(W_dropped, 0)
                W_sym = nx.from_numpy_matrix(W_dropped)
                if nx.is_connected(W_sym):
                    connected = True
                    W = np.copy(W_dropped)     
    elif graph_type == "cycle":
        if seed == -1:
            weights = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            weights = local_state.uniform(low=min_weight, high=max_weight, size=(n,n))
        W = np.zeros((n,n))
        i = 0
        j = 1
        for k in range(n-1):
            W[i,j] = weights[k]
            i+=1
            j+=1
        # Add last weight
        W[0, n-1] = weights[n-1]                
    elif graph_type == "regular_log(n)":
        degseq = np.repeat(np.floor(np.log(n)), n)
        adjMatr = AdjacencyMatrixDegree(degseq)
        if seed == -1:
            W = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            W = local_state.uniform(low=min_weight, high=max_weight, size=n)
        W = np.multiply(W,adjMatr)
    elif graph_type == "intercalate":
        if seed == -1:
            weights = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            weights = local_state.uniform(low=min_weight, high=max_weight, size=(n,n))
        adjMatr = np.zeros((n,n))
        # Fill diagonals above the main diagonal
        for k in range(intercalation):
            for i in range(n-k-1):
                adjMatr[i, i+k+1] = 1
        # Fill upper right corner
        for j in range(intercalation):
            for i in range(j+1):
                adjMatr[j-i, n-i-1]=1
        W = np.multiply(weights, adjMatr)
    elif graph_type == "modulo":
        if seed == -1:
            weights = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            weights = local_state.uniform(low=min_weight, high=max_weight, size=(n,n))
        adjMatr = np.zeros((n,n))
        # Create circle
        for i in range(n-1):
            adjMatr[i, i+1] = 1
        # Fill upper right corner
        adjMatr[0, n-1]=1
        # Add other connections
        # Fill diagonals above the main diagonal
        for i in range(n-modulo):
            adjMatr[i, i+modulo] = 1
        # Fill upper right corner
        for i in range(modulo):
            adjMatr[modulo-1-i, n-i-1]=1
        W = np.multiply(weights, adjMatr)
    
    # Copy upper diagonal to lower diagonal (only the upper perturbation counts)
    i_lower = np.tril_indices(n, -1)
    W[i_lower] = W.T[i_lower]
    # Make sure diagonal is empty
    np.fill_diagonal(W, 0)
    
    return W

def InitializeFlatGraph(n, min_weight, max_weight, dropout = 0, seed = -1, graph_type = "complete", intercalation = None, modulo = None):
    if graph_type == "complete":
        if seed == -1:
            W = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            W = local_state.uniform(low=min_weight, high=max_weight, size=n)
        if dropout>0:            
            connected = False
            while connected == False:
                if seed == -1:
                    D = np.random.choice([0,1], size=n, replace=True, p=[dropout, 1-dropout])
                else:
                    D = local_state.choice([0,1], size=n, replace=True, p=[dropout, 1-dropout])
                # Check if the dropped out matrix is connected
                # Make it a symmetric matrix
                D_sym = SymmetricMatrix(D)
                # Check if it resembles a connected graph
                D_sym = nx.from_numpy_matrix(D_sym)
                if nx.is_connected(D_sym):
                    connected = True
            # Once we have a conencted graph, add weights
            W = np.multiply(W,D)       
    elif graph_type == "cycle":
        if seed == -1:
            weights = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            weights = local_state.uniform(low=min_weight, high=max_weight, size=n)
        W = np.zeros((n,n))
        i = 0
        j = 1
        for k in range(n-1):
            W[i,j] = weights[k]
            i+=1
            j+=1
        # Add last weight
        W[0, n-1] = weights[n-1]
        i_lower = np.tril_indices(n, -1)
        W[i_lower] = W.T[i_lower]
        # Make sure diagonal is empty
        np.fill_diagonal(W, 0)
        W = np.copy(W[np.triu_indices_from(W, k = 1)])
    elif graph_type == "regular_log(n)":
        degseq = np.repeat(np.floor(np.log(n)), n)
        adjMatr = AdjacencyMatrixDegree(degseq)
        if seed == -1:
            W = np.random.uniform(low=min_weight, high=max_weight, size=n)
        else:
            local_state = np.random.RandomState(seed)
            W = local_state.uniform(low=min_weight, high=max_weight, size=n)
        W = np.multiply(W,adjMatr)
        W = np.copy(W[np.triu_indices_from(W, k = 1)])
    elif graph_type == "intercalate":
        W = InitializeGraph(n, min_weight, max_weight, dropout, seed, graph_type, intercalation)
        W = np.copy(W[np.triu_indices_from(W, k = 1)])
    elif graph_type == "modulo":
        W = InitializeGraph(n, min_weight, max_weight, dropout, seed, graph_type, None, modulo)
        W = np.copy(W[np.triu_indices_from(W, k = 1)])
    return W

def AdjacencyMatrixDegree(degseq):
    # n is number of vertices  
    n = len(degseq)
    mat = [[0] * n for i in range(n)] 
    for i in range(n): 
        for j in range(i + 1, n): 
            # For each pair of vertex decrement  
            # the degree of both vertex.  
            if (degseq[i] > 0 and degseq[j] > 0): 
                degseq[i] -= 1
                degseq[j] -= 1
                mat[i][j] = 1
                mat[j][i] = 1
    return mat

def GetAllPossibleAdjacencyFlatMatrixes(n):
    import itertools
    flatMatrixes = list(itertools.product([0, 1], repeat=n))
    return flatMatrixes

def GetKRandomPartition(n, k, seed = -1):
    partition = {}
    if seed == -1:
        for i in range(n):
            partition[i]=np.random.randint(low=0, high=k, size=1)[0]
    else:
        local_state = np.random.RandomState(seed)
        for i in range(n):
            partition[i]=local_state.randint(low=0, high=k, size=1)[0]
    return partition

# G stands for special ill-posed matrixes
def GetGWeightMatrix(g):
    if g == 1:
        G = np.array([[0, 7, 0, 0, 0, 0, 0, 0],
             [0, 0, 6, 0, 0, 0, 0, 0],
             [0, 0, 0, 5, 0, 0, 0, 0],
             [0, 0, 0, 0, 1, 3, 0, 0],
             [0, 0, 0, 0, 0, 0, 1, 1],
             [0, 0, 0, 0, 0, 0, 2, 0],
             [0, 0, 0, 0, 0, 0, 0, 0],
             [0, 0, 0, 0, 0, 0, 0, 0]]).astype(float)
    elif g == 2:
        G = np.array([
               [0, 51, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               [0, 0, 50, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 49, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 0, 8, 0, 10, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 8, 0, 8, 5, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 4, 0],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
               [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]).astype(float)
    else:
        raise("No")
    n = np.size(G, 1)
    i_lower = np.tril_indices(n, -1)
    G[i_lower] = G.T[i_lower]
    return G

def WeightsStats(W):
    print("Weight statistics\n")
    mean = []
    median = []
    std = []
    min_w = []
    max_w = []
    n = []
    W_without_0_edges = np.copy(W[W!=0])
    mean.append(W_without_0_edges.mean())
    median.append(np.median(W_without_0_edges))
    std.append(W_without_0_edges.std())
    min_w.append(W_without_0_edges.min())
    max_w.append(W_without_0_edges.max())
    s = W_without_0_edges.shape[0]
    s = int((1+np.sqrt(1+4*2*s))/2)+1
    n.append(s)
    print(tabulate(list(zip(*[n, mean, median, std, min_w, max_w])), 
                   headers=["n", 'mean', 'median', 'std', "min_weight", "max_weight"], floatfmt=".3f"))
    return

def Get0Partition(n):
    partition = {}
    for i in range(n):
        partition[i]=0
    return partition

def Get1Partition(n):
    partition = {}
    for i in range(n):
        partition[i]=1
    return partition

def GetStupidPartition(n):
    partition = {}
    for i in range(n):
        partition[i]=-1
    return partition

def GetInitialPartition(n, k, initial_partition_type, seed = -1):
    if initial_partition_type == "random":
        initial_partition = GetKRandomPartition(n, k, seed)
    if initial_partition_type == "0":
        initial_partition = Get0Partition(n)
    if initial_partition_type == "1":
        initial_partition = Get1Partition(n)
    return initial_partition

def CutCost(W, partition, n, k):
    z = 0
    identity = np.identity(k)
    for i in range(n):
        for j in range(i, n):
            new_edge = (1-identity[partition[i], partition[j]])*W[i,j]
            z += new_edge
    return z
        
def CutGainAfterFlip(W, initial_partition, final_partition, n):
    # Vi: initial partition of the vertex v
    # Vj: final partition of the vertex v
    # z: current cut
    weights_to_add = 0
    weights_to_substract = 0
    
    diff_partition = {key: initial_partition[key] - final_partition.get(key, 0) for key in initial_partition}
    
    # v: vertex that flipped
    v = [key for key, val in diff_partition.items() if val!=0][0]
    v_old_partition = initial_partition[v]
    v_new_partition = final_partition[v]
    
    # Vertices belonging to the old group and the new group
    vertices_in_old_partition = [key  for (key, value) in initial_partition.items() if value == v_old_partition]
    vertices_in_new_partition = [key  for (key, value) in initial_partition.items() if value == v_new_partition]

    for vi in vertices_in_old_partition:
        if vi == v:
            continue
        else:
            weights_to_add += W[v, vi]
    for vj in vertices_in_new_partition:
        weights_to_substract += W[v, vj]
    z = weights_to_add - weights_to_substract
    return z
            
def SolveMaxCut(W, n, k, z, initial_partition, heuristic, seed = -1):
    tol = 1e-8
    maxIter = 100000
    it = 0
    
    zt1 = -10000
    zt2 = CutCost(W, initial_partition, n, k)
    
    # In case the cut is already optimal
    new_z = zt2
    
    # Store old and new partition
    old_partition = GetStupidPartition(n)
    new_partition = initial_partition
    
    start = time.time()
    if heuristic == "GBF":
        # Greedy Best Flip
        while(old_partition!=new_partition and it<maxIter):
            old_partition = new_partition
            new_partition, new_z = GreedyBestFlip(W, new_partition, n, k, zt2)
            zt1 = zt2
            zt2 = new_z
            if old_partition!=new_partition:
                it = it + 1
    elif heuristic == "RPF":
        # Random Positive Flip
         while(old_partition!=new_partition and it<maxIter):
            old_partition = new_partition
            new_partition, new_z = RandomPositiveFlip(W, new_partition, n, k, zt2, seed)
            zt1 = zt2
            zt2 = new_z
            if old_partition!=new_partition:
                it = it + 1
    elif heuristic == "WF":
        # Worst Flip
         while(old_partition!=new_partition and it<maxIter):
            old_partition = new_partition
            new_partition, new_z = WorstFlip(W, new_partition, n, k, zt2)
            zt1 = zt2
            zt2 = new_z
            if old_partition!=new_partition:
                it = it + 1

    elif heuristic == "FNF":
        # First Next Flip
        # Only for testing purposes
        iters_fnf = 3
        for i in range(iters_fnf):
            partition, new_z = FirstNextFlip(W, partition, n, k, zt2)
            zt1 = zt2
            zt2 = new_z
            if old_partition!=new_partition:
                it = it + 1
            
    end = time.time()
    elapsed_time = np.round(end - start, 4)
    # The last step would return the same partition, so it-1
    return new_partition, new_z, elapsed_time, it

def SolveGlobalMaxCut(W, n, k):
    # Get all possible partitions
    import itertools
    k_vector = range(k)
    allPartitions = list(itertools.product(k_vector, repeat=n))
    n_partitions = len(allPartitions)
    max_cut = -100000
    best_partition = allPartitions[0]
    start = time.time()
    for partition in allPartitions:
        c = CutCost(W, partition, n, k)
        if c > max_cut:
            max_cut = c
            best_partition = partition
    end = time.time()
    elapsed_time = np.round(end - start, 4)
    # Create the partition dictionary
    best_partition = list(best_partition)
    keys = range(0, n)
    best_partition_d = {k:v for k,v in zip(keys, best_partition)}
    # The last step would return the same partition, so it-1
    return best_partition_d, max_cut, elapsed_time

def GurobiSolveMaxCut(W, n):
    # Initialize model
    m = gp.Model("max-cut")
    m.setParam('OutputFlag', 0)
    # Define variables
    # x: partition of each node?
    # z: edge is present in the cut?
    z = m.addVars(n, n, vtype=GRB.BINARY, name="edges")
    x = m.addVars(n, vtype=GRB.BINARY, name='partitions')
    
    
    # Add constrains
    m.addConstrs((z[i,j]<=x[i]+x[j] for i in range(n) for j in range(n)), "C1")
    m.addConstrs((z[i,j]+x[i]+x[j]<=2 for i in range(n) for j in range(n)), "C2")
    
    # Set objective function
    obj_f = gp.quicksum(W[(i,j)]*z[(i,j)] for i in range(n) for j in range(n))
    m.setObjective(obj_f, GRB.MAXIMIZE)
    
    # Go
    m.optimize()
    
    # Get solution
    best_cut = m.getObjective().getValue()
    
    # Get final partition
    nodes = m.getVars()
    best_partition = {}
    for i in range(n):
        best_partition[i] = nodes[i].x
    
    return best_partition, best_cut
    
def SymmetricMatrix(W):
    m = len(W)
    n = int((1+np.sqrt(1+4*2*m))/2)
    A = np.zeros(shape=(n, n))
    # k runs along W
    k = 0
    # Fill upper diagonal
    for i in range(n-1):
        for j in range(i+1, n):
            A[i,j]=W[k]
            k+=1
    # Fill lower diagonal
    i_lower = np.tril_indices(n, -1)
    A[i_lower] = A.T[i_lower]
    return A

def ScaleMatrix(W, a, b):
    non_zeros = np.nonzero(W)
    min_W = W[non_zeros].min()
    max_W = W.max()
    # Keep structure
    W_scaled = np.copy(W.astype(float))
    W_scaled[non_zeros] = (b-a)*((W[non_zeros]-min_W)/(max_W-min_W))+a
    return W_scaled

def NumpyToCsv(array, filename):
    np.savetxt(str(filename) + ".csv", array, delimiter=";")

def DeleteTempResults(filename):
    if os.path.exists(filename):
        os.remove(filename)
    return

def CsvToNumpy(filename):
    return np.genfromtxt(str(filename) + ".csv", delimiter=";")

def KolmogorovSmirnovTest(x1, x2, alpha):
    st, p = stats.ks_2samp(x1, x2)
    n1 = len(x1)
    n2 = len(x2)
    size_term = np.sqrt((n1+n2)/(n1*n2))
    if alpha == 0.05:
        d = size_term*1.36
    elif alpha == 0.025:
        d = size_term*1.48
    else:
        return
    if st < d:
        print("Both distributions are the same with probability", 100*(1-alpha), "%")
        print(st, "<", d)
    if st > d:
        print("Reject null hypothesis: different distributions with probability", 100*(1-alpha), "%")
        print(st, ">", d)
    return
    
# Meshgrid - No need to parallelize
def MeshGrid3dMaxCut(k, min_weight, max_weight, initial_partition_type, n_points_ax, n_iters, complexity, heuristic, sigma):
    weights_size = 3
    x = np.zeros(shape=(np.power(n_points_ax+1, 3), weights_size))
    f = np.zeros(np.power(n_points_ax+1, 3))
    # p runs over all points
    p = 0
    for i in range(n_points_ax+1):
        x_val = min_weight + (max_weight-min_weight)*i/n_points_ax
        for j in range(n_points_ax+1):
            y_val = min_weight + (max_weight-min_weight)*j/n_points_ax
            for l in range(n_points_ax+1):
                z_val = min_weight + (max_weight-min_weight)*l/n_points_ax
                w_flat = [x_val, y_val, z_val]
                w = SymmetricMatrix(w_flat)
                steps = np.zeros(n_iters)
                for it in range(n_iters):
                    initial_partition = GetInitialPartition(3, k, initial_partition_type)
                    initial_z = CutCost(w, initial_partition, 3, k)
                    if complexity == "average":
                        _p, _z, _t, steps[it] = SolveMaxCut(w, 3, k, initial_z, initial_partition, heuristic)
                    elif complexity == "smoothed":
                        # n_iters is also used for n_perturbations
                        steps[it] = SmoothedComplexity(w_flat, 3, k, initial_partition_type, 0, n_iters, heuristic, sigma)
                f[p] = np.mean(steps)
                x[p] = [x_val, y_val, z_val]
                p+=1
    return x, f

def PlotMeshGrid3d(x, f, title, colorsMap='jet'):
    cm = plt.get_cmap(colorsMap)
    cNorm = matcolors.Normalize(vmin=min(f), vmax=max(f))
    scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cm)
    fig = plt.figure()
    ax = Axes3D(fig)
    ax.scatter(x[:,0], x[:,1], x[:,2], c=scalarMap.to_rgba(f))
    plt.xticks(np.arange(min(x[:,0]), max(x[:,0])+1, 0.5))
    plt.yticks(np.arange(min(x[:,1]), max(x[:,1])+1, 0.5))
    scalarMap.set_array(f)
    fig.colorbar(scalarMap)
    if title:
        plt.title(title)
    plt.show()

# Plot results
def GetColorsVector(n):
    all_colors = ["blue", "red", "green", "black", "purple"]
    #"orange", "green", "red", "black", "purple", "grey"]
    return all_colors[:n]

def PlotGraph(W, partition = None, ax=None):
    G = nx.from_numpy_matrix(np.round(W, 3))
    # Relabel nodes
    #mapping = {0: '1', 1: '2', 2: '3'}
    #G = nx.relabel_nodes(G, mapping)
    pos = nx.circular_layout(G)
    
    # Create edge labels
    labels = nx.get_edge_attributes(G, 'weight')
    edges, weights = zip(*nx.get_edge_attributes(G,'weight').items())
    
    # Draw the graph according to node positions
    if partition is not None:
        colors = np.fromiter(partition.values(), dtype=int)
        if ax is None:
            nx.draw(G, pos, with_labels=True, node_color=colors, edgelist=edges, edge_color=weights, width=5.0, edge_cmap=plt.cm.Blues)
        else:
            nx.draw(G, pos, with_labels=True, node_color=colors, edgelist=edges, edge_color=weights, width=5.0, edge_cmap=plt.cm.Blues, ax=ax)
    else:
        if ax is None:
            nx.draw(G, pos, with_labels=True, edgelist=edges, edge_color=weights, width=5.0, edge_cmap=plt.cm.Blues)
        else:
            nx.draw(G, pos, with_labels=True, edgelist=edges, edge_color=weights, width=5.0, edge_cmap=plt.cm.Blues, ax=ax)
    # Draw edge labels according to node positions
    #nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)
    if ax is None:
        plt.show()    
    return

def PlotWeightsHistogram(W):
    # Start plot
    plt.figure(1)
    plt.subplot(211)
    # Plot histogram of weights
    W_without_0_edges = np.copy(W[W!=0])
    plt.hist(W_without_0_edges, bins=20, color='blue')
    plt.title('Weight distribution', fontsize=8)
    # Do KSTest with uniform
    n = len(W)
    uniform_sample = np.random.uniform(low=-1, high=1, size=n)
    KolmogorovSmirnovTest(W, uniform_sample, alpha=0.025)
    # Histogram of the sum of the edges from each vertex
    sum_of_edges = np.sum(SymmetricMatrix(W), axis = 0)
    plt.subplot(212)
    plt.hist(sum_of_edges, bins=10)
    plt.title('Sum of edges', fontsize=8)
    plt.tight_layout()
    plt.show()
    # Do KSTest with artificial sample (assume independency)
    #artificial_sample = np.zeros(n)
    #for i in range(n):
    #    artificial_sample[i] = np.sum(np.random.uniform(low=-1, high=1, size=n-1))
    #KolmogorovSmirnovTest(sum_of_edges, artificial_sample, 0.05)
    return

def PlotRegressionResults(nodes, steps_mean, steps_sd, with_errorbars, r_var, r_var_label, method):
    
    # Convert X axis according to method
    x = nodes
    if method=="polynomial":
        x = np.log(x.reshape(-1, 1))
    if method == "quasipolynomial":
        x = np.log(x.reshape(-1, 1))*x.reshape(-1, 1)

    cmap = GetColorsVector(len(r_var))
    
    # t runs over all columns of steps_mean
    t=0
    slopes=[]
    intercepts=[]
    r_values=[]
    p_values=[]
    std_errs=[]

    # Create a new plot
    plt.figure(np.random.randint(501, 1000))
    
    for r_v in r_var:
        # Convert Y axis according to method
        y = steps_mean[:,t]
        if (method=="polynomial") or (method=="exponential") or (method=="quasipolynomial"):
            y_log = np.log(y)
        plt.scatter(x, y_log, c=cmap[t])
        if with_errorbars:
            y_sd = steps_sd[:, t]/y
            plt.errorbar(x,y_log,yerr=y_sd, linestyle="None", c=cmap[t])

        # Linear regression
        slope, intercept, r_value, p_value, std_err = stats.mstats.linregress(x,y_log)

        # Store results
        slopes.append(slope)
        intercepts.append(intercept)
        r_values.append(r_value)
        p_values.append(p_value)
        std_errs.append(std_err)

        # Linear regression prediction
        y_reg = intercept + slope*x
        plt.plot(x, y_reg, label=str(r_var_label) + '=' + str(r_v), c=cmap[t])
        t+=1

    if method=="polynomial":
        plt.title('Polynomial behavior')     
        plt.xlabel('ln(Nodes)') 
        plt.ylabel('ln(Steps)')
    
    if method=="exponential":
        plt.title('Exponential behavior')     
        plt.xlabel('Nodes') 
        plt.ylabel('ln(Steps)')
        
    if method=="quasipolynomial":
        plt.title('Quasiplynomial behavior')     
        plt.xlabel('ln(Nodes)*Nodes') 
        plt.ylabel('ln(Steps)')
    
    plt.legend()        
    plt.show()

    # Print table
    print("Regression results:\n")
    print(tabulate(list(zip(*[r_var, slopes, intercepts, r_values])), headers=[r_var_label, 'slope', 'intercept', 'r_value'], floatfmt=".3f"))
    
    return slopes, intercepts
    
def PlotResults(nodes, steps_mean, steps_sd, with_errorbars, r_var, r_var_label, xlabel = "Nodes", ylabel = "Steps"):
    #r_var is running variable
    
    # Show results for each r_var
    x = nodes.reshape(-1, 1)

    # Legend
    r_var_labels=[]

    # t runs over all columns of steps_mean
    t=0

    cmap = GetColorsVector(len(r_var))
    
    # Create a new plot
    plt.figure(np.random.randint(0, 500))
    
    for r_v in r_var:
        run_steps = steps_mean[:,t]
        plt.scatter(x, run_steps, c=cmap[t])
        if with_errorbars:
            run_steps_sd = steps_sd[:, t]
            plt.errorbar(x,run_steps,yerr=run_steps_sd, linestyle="None", c=cmap[t])
        r_var_labels.append(str(r_var_label) + "=" + str(r_v))
        t+=1

    plt.title(str(xlabel) + " vs. " + str(ylabel)) 
    plt.xlabel(xlabel) 
    plt.ylabel(ylabel)
    plt.legend(r_var_labels)
    plt.show()
    
def PlotResultsSmoothed(nodes, steps_mean, steps_sd, with_errorbars, r_var, r_var_label, sigma, withUpperBound):
    #r_var is running variable
    
    # Show results for each r_var
    x = nodes.reshape(-1, 1)

    # Legend
    r_var_labels=[]

    # t runs over all columns of steps_mean
    t=0
    
    # Create a new plot
    plt.figure(np.random.randint(1001, 1500))

    for r_v in r_var:
        run_steps = steps_mean[:,t]
        plt.scatter(x, run_steps)
        if with_errorbars:
            run_steps_sd = steps_sd[:, t]
            plt.errorbar(x,run_steps,yerr=run_steps_sd, linestyle="None")
        r_var_labels.append(str(r_var_label) + "=" + str(r_v))
        t+=1
    # Assume gaussian
    if withUpperBound:
        phi = 1/np.sqrt(2*np.pi*sigma*sigma)
        upper_bound = phi*np.power(nodes, 7.83)
        plt.plot(nodes, upper_bound, color='red', linewidth=1.0, linestyle='--')
    plt.title('Steps vs. Nodes') 
    plt.xlabel('Nodes') 
    plt.ylabel('Steps')
    plt.legend(r_var_labels)
    plt.show()
    return

def PlotTwoRegressionResults(nodes1, steps_mean1, steps_sd1, nodes2, steps_mean2, steps_sd2, with_errorbars, r_var, r_var_label, method, labels):
    
    # Create a new plot
    plt.figure(np.random.randint(501, 1000))

    # t runs over all columns of steps_mean
    t=0
    slopes1=[]
    intercepts1=[]
    r_values1=[]
    p_values1=[]
    std_errs1=[]    
    # Convert X axis according to method
    x1 = nodes1
    if method=="polynomial":
        x1 = np.log(x1.reshape(-1, 1))
    if method == "quasipolynomial":
        x1 = np.log(x1.reshape(-1, 1))*x1.reshape(-1, 1)
    
    # For the averae case, we have to reduce the number of vertices so that the have similar lengths
    last_index = np.argmax(nodes1>np.max(nodes2))
    if last_index == 0:
        last_index = len(x1-1)
    x1 = x1[:last_index]
    steps_mean1 = steps_mean1[:last_index]
    steps_sd1 = steps_sd1[:last_index]
    
    for r_v in r_var:
        # Convert Y axis according to method
        y1 = steps_mean1[:,t]
        if (method=="polynomial") or (method=="exponential") or (method=="quasipolynomial"):
            y1 = np.log(y1)
        plt.scatter(x1, y1, c = "darkred", alpha=0.5)
        if with_errorbars:
            if len(steps_sd1)>0:
                y_sd1 = steps_sd1[:, t]
                plt.errorbar(x1,y1,yerr=y_sd1, linestyle="None", c = "darkred", alpha=0.5)

        # Linear regression
        slope1, intercept1, r_value1, p_value1, std_err1 = stats.mstats.linregress(x1,y1)

        # Store results
        slopes1.append(slope1)
        intercepts1.append(intercept1)
        r_values1.append(r_value1)
        p_values1.append(p_value1)
        std_errs1.append(std_err1)

        # Linear regression prediction
        y_reg1 = intercept1 + slope1*x1
        plt.plot(x1, y_reg1, label=labels[0], c = "red")
        t+=1
        
    # t runs over all columns of steps_mean
    t=0
    slopes2=[]
    intercepts2=[]
    r_values2=[]
    p_values2=[]
    std_errs2=[]   
    
    # X2 is smoothed, therefore no error bars
    # Convert X axis according to method
    x2 = nodes2
    if method=="polynomial":
        x2 = np.log(x2.reshape(-1, 1))
    if method == "quasipolynomial":
        x2 = np.log(x2.reshape(-1, 1))*x2.reshape(-1, 1)
    
    for r_v in r_var:
        # Convert Y axis according to method
        y2 = steps_mean2[:,t]
        if (method=="polynomial") or (method=="exponential") or (method=="quasipolynomial"):
            y2 = np.log(y2)
        plt.scatter(x2, y2, c = "darkblue", alpha=0.5)
        if with_errorbars:
            if len(steps_sd2)>0:
                y_sd2 = steps_sd2[:, t]
                plt.errorbar(x2,y2,yerr=y_sd2, linestyle="None", c = "darkblue", alpha=0.5)

        # Linear regression
        slope2, intercept2, r_value2, p_value2, std_err2 = stats.mstats.linregress(x2,y2)

        # Store results
        slopes2.append(slope2)
        intercepts2.append(intercept2)
        r_values2.append(r_value2)
        p_values2.append(p_value2)
        std_errs2.append(std_err2)

        # Linear regression prediction
        y_reg2 = intercept2 + slope2*x2
        plt.plot(x2, y_reg2, label=labels[1], c = "blue")
        t+=1

    if method=="polynomial":
        plt.title('Polynomial behavior')     
        plt.xlabel('ln(Nodes)') 
        plt.ylabel('ln(Steps)')
    
    if method=="exponential":
        plt.title('Exponential behavior')     
        plt.xlabel('Nodes') 
        plt.ylabel('ln(Steps)')
        
    if method=="quasipolynomial":
        plt.title('Quasiplynomial behavior')     
        plt.xlabel('ln(Nodes)*Nodes') 
        plt.ylabel('ln(Steps)')
    
    plt.legend(loc="upper left")        
    plt.show()

    # Print table
    print("Regression results:\n")
    print(labels[0])
    print(tabulate(list(zip(*[r_var, slopes1, intercepts1, r_values1])), headers=[r_var_label, 'slope', 'intercept', 'r_value'], floatfmt=".3f"))
    print(labels[1])
    print(tabulate(list(zip(*[r_var, slopes2, intercepts2, r_values2])), headers=[r_var_label, 'slope', 'intercept', 'r_value'], floatfmt=".3f"))
    
    return

def PlotThreeRegressionResults(nodes1, steps_mean1, steps_sd1, nodes2, steps_mean2, steps_sd2, nodes3, steps_mean3, steps_sd3,
                               with_errorbars, r_var, r_var_label, method, labels):
    
    # Create a new plot
    plt.figure(np.random.randint(501, 1000))

    # t runs over all columns of steps_mean
    t=0
    slopes1=[]
    intercepts1=[]
    r_values1=[]
    p_values1=[]
    std_errs1=[]    
    # Convert X axis according to method
    x1 = nodes1
    if method=="polynomial":
        x1 = np.log(x1.reshape(-1, 1))
    if method == "quasipolynomial":
        x1 = np.log(x1.reshape(-1, 1))*x1.reshape(-1, 1)
    
    # For the averae case, we have to reduce the number of vertices so that the have similar lengths
    last_index = np.argmax(nodes1>np.max(nodes2))
    if last_index == 0:
        last_index = len(x1-1)
    x1 = x1[:last_index]
    steps_mean1 = steps_mean1[:last_index]
    steps_sd1 = steps_sd1[:last_index]
    
    # Convert Y axis according to method
    y1 = steps_mean1
    if (method=="polynomial") or (method=="exponential") or (method=="quasipolynomial"):
        y1 = np.log(y1)
    plt.scatter(x1, y1, c = "darkred", alpha=0.5)
    if with_errorbars:
        if len(steps_sd1)>0:
            y_sd1 = steps_sd1
            plt.errorbar(x1,y1,yerr=y_sd1, linestyle="None", c = "darkred", alpha=0.5)

    # Linear regression
    slope1, intercept1, r_value1, p_value1, std_err1 = stats.mstats.linregress(x1,y1)

    # Store results
    slopes1.append(slope1)
    intercepts1.append(intercept1)
    r_values1.append(r_value1)
    p_values1.append(p_value1)
    std_errs1.append(std_err1)

    # Linear regression prediction
    y_reg1 = intercept1 + slope1*x1
    plt.plot(x1, y_reg1, label=labels[0], c = "red")
        
    # t runs over all columns of steps_mean
    slopes2=[]
    intercepts2=[]
    r_values2=[]
    p_values2=[]
    std_errs2=[]   
    
    # X2 is smoothed, therefore no error bars
    # Convert X axis according to method
    x2 = nodes2
    if method=="polynomial":
        x2 = np.log(x2.reshape(-1, 1))
    if method == "quasipolynomial":
        x2 = np.log(x2.reshape(-1, 1))*x2.reshape(-1, 1)
    
    # Convert Y axis according to method
    y2 = steps_mean2
    if (method=="polynomial") or (method=="exponential") or (method=="quasipolynomial"):
        y2 = np.log(y2)
    plt.scatter(x2, y2, c = "darkblue", alpha=0.5)
    if with_errorbars:
        if len(steps_sd2)>0:
            y_sd2 = steps_sd2
            plt.errorbar(x2,y2,yerr=y_sd2, linestyle="None", c = "darkblue", alpha=0.5)

    # Linear regression
    slope2, intercept2, r_value2, p_value2, std_err2 = stats.mstats.linregress(x2,y2)

    # Store results
    slopes2.append(slope2)
    intercepts2.append(intercept2)
    r_values2.append(r_value2)
    p_values2.append(p_value2)
    std_errs2.append(std_err2)

    # Linear regression prediction
    y_reg2 = intercept2 + slope2*x2
    plt.plot(x2, y_reg2, label=labels[1], c = "blue")
        
    # t runs over all columns of steps_mean
    slopes3=[]
    intercepts3=[]
    r_values3=[]
    p_values3=[]
    std_errs3=[]   
    
    # Convert X axis according to method
    x3 = nodes3
    if method=="polynomial":
        x3 = np.log(x3.reshape(-1, 1))
    if method == "quasipolynomial":
        x3 = np.log(x3.reshape(-1, 1))*x3.reshape(-1, 1)
    
    # Convert Y axis according to method
    y3 = steps_mean3
    if (method=="polynomial") or (method=="exponential") or (method=="quasipolynomial"):
        y3 = np.log(y3)
    plt.scatter(x3, y3, c = "darkgreen", alpha=0.5)
    if with_errorbars:
        if len(steps_sd3)>0:
            y_sd3 = steps_sd3
            plt.errorbar(x3,y3,yerr=y_sd3, linestyle="None", c = "darkgreen", alpha=0.5)

    # Linear regression
    slope3, intercept3, r_value3, p_value3, std_err3 = stats.mstats.linregress(x3,y3)

    # Store results
    slopes3.append(slope3)
    intercepts3.append(intercept3)
    r_values3.append(r_value3)
    p_values3.append(p_value3)
    std_errs3.append(std_err3)

    # Linear regression prediction
    y_reg3 = intercept3 + slope3*x3
    plt.plot(x3, y_reg3, label=labels[2], c = "green")

    if method=="polynomial":
        plt.title('Polynomial behavior')     
        plt.xlabel('ln(Nodes)') 
        plt.ylabel('ln(Steps)')
    
    if method=="exponential":
        plt.title('Exponential behavior')     
        plt.xlabel('Nodes') 
        plt.ylabel('ln(Steps)')
        
    if method=="quasipolynomial":
        plt.title('Quasiplynomial behavior')     
        plt.xlabel('ln(Nodes)*Nodes') 
        plt.ylabel('ln(Steps)')
        
    if method == "lineal":
        plt.title('Ratios')     
        plt.ylabel('local/global') 
        plt.xlabel('Nodes')
    
    plt.legend(loc="lower left")        
    plt.show()

    # Print table
    print("Regression results:\n")
    print(tabulate(list(zip(*[labels[0], slopes1, intercepts1, r_values1])), headers=[r_var_label, 'slope', 'intercept', 'r_value'], floatfmt=".3f"))
    print(tabulate(list(zip(*[labels[1], slopes2, intercepts2, r_values2])), headers=[r_var_label, 'slope', 'intercept', 'r_value'], floatfmt=".3f"))
    print(tabulate(list(zip(*[labels[2], slopes3, intercepts3, r_values3])), headers=[r_var_label, 'slope', 'intercept', 'r_value'], floatfmt=".3f"))
    
    return

def MultilinearRegression(X, Y, method):
    import statsmodels.api as sm
    from sklearn import linear_model
    if method == "log-log":
        X = np.log(X)
        Y = np.log(Y)
    X = sm.add_constant(X) # adding a constant
    model = sm.OLS(Y, X).fit()
    #predictions = model.predict(X) 
    print_model = model.summary()
    print(print_model)
    return model.params, model.rsquared

def GreedyBestFlip(W, partition, n, k, z):
    cut_costs = []
    partitions_flip = []
    # Permutate vertices so that we do not always start by the same one
    order = np.random.permutation(range(n))
    for i in order:
        for j in range(1, k):            
            new_partition = partition.copy()
            new_partition[i] = (new_partition[i]+j)%k
            partitions_flip.append(new_partition)
            cut_costs.append(CutGainAfterFlip(W, partition, new_partition, n))
      
    # Convert the list to a numpy array
    cut_costs = np.asarray(cut_costs)
    if np.any(cut_costs[cut_costs>0]):
        # New best partition was found
        best_index = np.argmax(cut_costs)
        best_partition = partitions_flip[best_index]
        new_cut_cost = z + cut_costs[best_index]
        return best_partition, new_cut_cost
    else:
        # We are in a local optimum
        return partition, z
    
def RandomPositiveFlip(W, partition, n, k, z, seed = -1):
    # Permutate vertices so that we do not always start by the same one
    # Randomness
    if seed == -1:
        order = np.random.permutation(range(n))
    else:
        local_state = np.random.RandomState(seed)
        order = local_state.permutation(range(n))
    for i in order:
        for j in range(1, k):            
            new_partition = partition.copy()
            new_partition[i] = (new_partition[i]+j)%k
            new_z = CutGainAfterFlip(W, partition, new_partition, n)
            if new_z > 0:
                return new_partition, z + new_z
      
    return partition, z

def WorstFlip(W, partition, n, k, z):
    cut_costs = []
    partitions_flip = []
    # Permutate vertices so that we do not always start by the same one
    order = np.random.permutation(range(n))
    for i in order:
        for j in range(1, k):            
            new_partition = partition.copy()
            new_partition[i] = (new_partition[i]+j)%k
            partitions_flip.append(new_partition)
            cut_costs.append(CutGainAfterFlip(W, partition, new_partition, n))
      
    # Convert the list to a numpy array
    cut_costs = np.asarray(cut_costs)
    
    if np.any(cut_costs[cut_costs>0]):
        # A better partition was found
        min_val = min(c for c in cut_costs if c > 0)
        worst_index = np.argwhere(cut_costs==min_val)[0][0]
        worst_partition = partitions_flip[worst_index]
        new_cut_cost = z + cut_costs[worst_index]
        return worst_partition, new_cut_cost
    else:
        # We are in a local optimum
        return partition, z

def RunGridMaxCutAverageK(min_nodes, max_nodes, step_nodes, initial_partition_type, dropout, ks, min_weight, max_weight, iters_for_nk, heuristic, graph_type = "complete", storeCSV=False):

    # Create nodes grid
    # nodes = np.linspace(min_nodes, max_nodes, num=step_nodes, dtype=int)
    nodes = np.logspace(np.log(min_nodes), np.log(max_nodes), num=step_nodes, base=np.exp(1))
    # Remove decimal part
    nodes = np.floor(nodes)
    # Convert to integer
    nodes = nodes.astype(int)

    steps_mean = np.zeros((len(nodes), len(ks)))
    steps_sd = np.zeros((len(nodes), len(ks)))
    
    # Save nodes
    if storeCSV:
        NumpyToCsv(nodes, "nodes")
    
    i = 0
    j = 0

    for ni in nodes:
        weights_size = int(ni*(ni-1)/2)
        for ki in ks:
            steps = np.zeros(iters_for_nk)
            for it in range(iters_for_nk):
                # Create graph and initial partition
                W_k_it = InitializeFlatGraph(weights_size, min_weight, max_weight, dropout, graph_type=graph_type)
                W_k_it = SymmetricMatrix(W_k_it)
                initial_partition = GetInitialPartition(ni, ki, initial_partition_type)
                # Get initial cost value
                initial_z = CutCost(W_k_it, initial_partition, ni, ki)
                # Get next local maximum
                partition, z, elapsed_time, n_steps = SolveMaxCut(W_k_it, ni, ki, initial_z, initial_partition, heuristic)
                # Save results for each iteration
                steps[it]=n_steps
             
            # Save results for a i,j combination    
            steps_mean[i,j] = np.mean(steps)
            steps_sd[i,j] = np.std(steps)/np.sqrt(iters_for_nk)
            if storeCSV:
                DeleteTempResults("average_complexities.csv")
                NumpyToCsv(steps_mean, "average_complexities")
                DeleteTempResults("average_complexities_sd.csv")
                NumpyToCsv(steps_sd, "average_complexities_sd")
            j = j + 1
        j = 0
        i = i + 1
    return steps_mean, steps_sd, nodes

def FindWorstInstance(n, initial_partition_type, dropout, k, min_weight, max_weight, iters_for_nk, heuristic, storeCSV=False):
    steps = np.zeros(iters_for_nk)   
    weights_size = int(n*(n-1)/2)
    max_value = 0
    for it in range(iters_for_nk):
        # Create graph and initial partition
        W_k_it = InitializeFlatGraph(weights_size, min_weight, max_weight, dropout)
        W_k_it = SymmetricMatrix(W_k_it)
        initial_partition = GetInitialPartition(n, k, initial_partition_type)
        # Get initial cost value
        initial_z = CutCost(W_k_it, initial_partition, n, k)
        # Get next local maximum
        partition, z, elapsed_time, n_steps = SolveMaxCut(W_k_it, n, k, initial_z, initial_partition, heuristic)
        # Save results for each iteration
        steps[it]=n_steps            
        if n_steps>max_value:
            max_value = n_steps
            worst_instance = W_k_it
            
    if storeCSV:
        DeleteTempResults("n_steps.csv")
        NumpyToCsv(steps, "n_steps")
        DeleteTempResults("worst_instance.csv")
        NumpyToCsv(worst_instance, "worst_instance")
    return n, steps

def RunGridMaxCutAverageDropout(min_nodes, max_nodes, step_nodes, initial_partition_type, dropouts, k, min_weight, max_weight, iters_for_nk, heuristic):

    # Create nodes grid
    # nodes = np.linspace(min_nodes, max_nodes, num=step_nodes, dtype=int)
    nodes = np.logspace(np.log(min_nodes), np.log(max_nodes), num=step_nodes, base=np.exp(1))
    # Remove decimal part
    nodes = np.floor(nodes)
    # Convert to integer
    nodes = nodes.astype(int)

    steps_mean = np.zeros((len(nodes), len(dropouts)))
    steps_sd = np.zeros((len(nodes), len(dropouts)))
    
    i = 0
    j = 0

    for ni in nodes:
        for d in dropouts:
            steps = np.zeros(iters_for_nk)
            for it in range(iters_for_nk):
                # Create graph and initial partition
                W = InitializeGraph(ni, min_weight, max_weight, d)
                initial_partition = GetInitialPartition(ni, k, initial_partition_type)
                # Get initial cost value
                initial_z = CutCost(W, initial_partition, ni, k)
                # Get next local maximum
                partition, z, elapsed_time, n_steps = SolveMaxCut(W, ni, k, initial_z, initial_partition, heuristic)
                # Save results for each iteration
                steps[it]=n_steps
                
            # Save results for a i,j combination    
            steps_mean[i,j] = np.mean(steps)
            steps_sd[i,j] = np.std(steps)/np.sqrt(iters_for_nk)
            j = j + 1
        j = 0
        i = i + 1
    return steps_mean, steps_sd, nodes

def RunGridMaxCutAverageDegrees(min_nodes, max_nodes, step_nodes, initial_partition_type, degrees, k, min_weight, max_weight, iters_for_nk, heuristic):

    # Create nodes grid
    # nodes = np.linspace(min_nodes, max_nodes, num=step_nodes, dtype=int)
    nodes = np.logspace(np.log(min_nodes), np.log(max_nodes), num=step_nodes, base=np.exp(1))
    # Remove decimal part
    nodes = np.floor(nodes)
    # Convert to integer
    nodes = nodes.astype(int)

    steps_mean = np.zeros((len(nodes), len(dropouts)))
    steps_sd = np.zeros((len(nodes), len(dropouts)))
    
    i = 0
    j = 0

    for ni in nodes:
        for d in degrees:
            steps = np.zeros(iters_for_nk)
            for it in range(iters_for_nk):
                # Create graph and initial partition
                W = InitializeGraph(ni, min_weight, max_weight, d)
                initial_partition = GetInitialPartition(ni, k, initial_partition_type)
                # Get initial cost value
                initial_z = CutCost(W, initial_partition, ni, k)
                # Get next local maximum
                partition, z, elapsed_time, n_steps = SolveMaxCut(W, ni, k, initial_z, initial_partition, heuristic)
                # Save results for each iteration
                steps[it]=n_steps
                
            # Save results for a i,j combination    
            steps_mean[i,j] = np.mean(steps)
            steps_sd[i,j] = np.std(steps)/np.sqrt(iters_for_nk)
            j = j + 1
        j = 0
        i = i + 1
    return steps_mean, steps_sd, nodes

def RunGridMaxCutAverageHeuristics(min_nodes, max_nodes, step_nodes, initial_partition_type, dropout, k, min_weight, max_weight, iters_for_nk, heuristics):

    # Create nodes grid
    # nodes = np.linspace(min_nodes, max_nodes, num=step_nodes, dtype=int)
    nodes = np.logspace(np.log(min_nodes), np.log(max_nodes), num=step_nodes, base=np.exp(1))
    # Remove decimal part
    nodes = np.floor(nodes)
    # Convert to integer
    nodes = nodes.astype(int)

    steps_mean = np.zeros((len(nodes), len(heuristics)))
    steps_sd = np.zeros((len(nodes), len(heuristics)))
    
    max_cut_mean = np.zeros((len(nodes), len(heuristics)))
    max_cut_sd = np.zeros((len(nodes), len(heuristics)))
    
    i = 0
    j = 0

    for ni in nodes:
        for h in heuristics:
            steps = np.zeros(iters_for_nk)
            max_cuts = np.zeros(iters_for_nk)
            for it in range(iters_for_nk):
                # Create graph and initial partition
                W = InitializeGraph(ni, min_weight, max_weight, dropout)
                initial_partition = GetInitialPartition(ni, k, initial_partition_type)
                # Get initial cost value
                initial_z = CutCost(W, initial_partition, ni, k)
                # Select method
                partition, z, elapsed_time, n_steps = SolveMaxCut(W, ni, k, initial_z, initial_partition, h)
                # Save results for each iteration
                steps[it]=n_steps
                max_cuts[it] = z
                
            # Save results for a i,j combination    
            steps_mean[i,j] = np.mean(steps)
            steps_sd[i,j] = np.std(steps)/np.sqrt(iters_for_nk)
            max_cut_mean[i,j] = np.mean(max_cuts)
            max_cut_sd[i,j] = np.std(max_cuts)/np.sqrt(iters_for_nk)
            j = j + 1
        j = 0
        i = i + 1
    return steps_mean, steps_sd, max_cut_mean, max_cut_sd, nodes

def RunGridMaxCutAverageIntercalations(min_nodes, max_nodes, step_nodes, initial_partition_type, dropout, k, min_weight, max_weight, iters_for_nk, heuristic, intercalations, storeCSV=False):

    # Create nodes grid
    # nodes = np.linspace(min_nodes, max_nodes, num=step_nodes, dtype=int)
    nodes = np.logspace(np.log(min_nodes), np.log(max_nodes), num=step_nodes, base=np.exp(1))
    # Remove decimal part
    nodes = np.floor(nodes)
    # Convert to integer
    nodes = nodes.astype(int)

    steps_mean = np.zeros((len(nodes), len(intercalations)))
    steps_sd = np.zeros((len(nodes), len(intercalations)))
    
    # Save nodes
    if storeCSV:
        NumpyToCsv(nodes, "nodes")
    i = 0
    j = 0

    for ni in nodes:
        weights_size = int(ni*(ni-1)/2)
        for intercalation in intercalations:
            real_intercalation = np.floor(intercalation*ni).astype(int)
            steps = np.zeros(iters_for_nk)
            for it in range(iters_for_nk):
                # Create graph and initial partition
                W_k_it = InitializeFlatGraph(weights_size, min_weight, max_weight, dropout, None, "intercalate", real_intercalation)
                W_k_it = SymmetricMatrix(W_k_it)
                initial_partition = GetInitialPartition(ni, k, initial_partition_type)
                # Get initial cost value
                initial_z = CutCost(W_k_it, initial_partition, ni, k)
                # Get next local maximum
                partition, z, elapsed_time, n_steps = SolveMaxCut(W_k_it, ni, k, initial_z, initial_partition, heuristic)
                # Save results for each iteration
                steps[it]=n_steps
             
            # Save results for a i,j combination    
            steps_mean[i,j] = np.mean(steps)
            steps_sd[i,j] = np.std(steps)/np.sqrt(iters_for_nk)
            if storeCSV:
                DeleteTempResults("average_complexities.csv")
                NumpyToCsv(steps_mean, "average_complexities")
                DeleteTempResults("average_complexities_sd.csv")
                NumpyToCsv(steps_sd, "average_complexities_sd")
            j = j + 1
        j = 0
        i = i + 1
    return  nodes, steps_mean, steps_sd,

#GLOBAL
def SolveGlobalAndLocalMaxCut(nodes, iters, min_weight, max_weight, heuristics, k, initial_partition_type, dropout, storeCSV=False):
    # Average value for each node (global)
    global_average_value = np.zeros((len(nodes), 1))
    global_average_value_sd = np.zeros((len(nodes), 1))
    
    # Average value for each node (local) and heuristic
    local_average_value = np.zeros((len(nodes), len(heuristics)))
    local_average_value_sd = np.zeros((len(nodes), len(heuristics)))
    
    # Avg(local/global) for each node and heuristic
    average_ratios = np.zeros((len(nodes), len(heuristics)))
    average_ratios_sd = np.zeros((len(nodes), len(heuristics)))
    
    # Save nodes
    if storeCSV:
        NumpyToCsv(nodes, "nodes")
    # Save lengths
    nodes_length = len(nodes)
    heuristics_length = len(heuristics)
    
    # Start loop
    for i in range(nodes_length):
        # Get current node
        n = int(nodes[i])
        
        # Get initial partition
        initial_partition = GetInitialPartition(n, k, initial_partition_type)
        
        # Create variable to store results for each iteration
        global_value_iters = np.zeros((iters, 1))
        local_value_iters = np.zeros((iters, len(heuristics)))
        ratios_iters = np.zeros((iters, len(heuristics)))
        
        # Start loop for each iteration
        for it in range(iters):
            # Initialize graph
            W = InitializeGraph(n, min_weight, max_weight, dropout)
            # Global result
            _p, global_value_iter = GurobiSolveMaxCut(W, n)
            global_value_iters[it] = global_value_iter
            # Local result for each heuristic
            initial_cut = CutCost(W, initial_partition, n, k)
            #print("Global value", global_value_iter)
            for h in range(heuristics_length):
                heuristic = heuristics[h]
                _p, local_value_iter_heuristic, _t, _it = SolveMaxCut(W, n, k, initial_cut, initial_partition, heuristic)
                local_value_iters[it,h] = local_value_iter_heuristic
                #print("Heuristic", heuristic, local_value_iter_heuristic)
                if global_value_iter == 0:
                    ratios_iters[it,h] = 1
                else:
                    ratios_iters[it,h] = local_value_iter_heuristic/global_value_iter
              
        # Get the average values and append them
        global_average_value[i] = np.mean(global_value_iters)
        local_average_value[i] = np.mean(local_value_iters, axis = 0)
        average_ratios[i] = np.mean(ratios_iters, axis = 0)
        
        # Get the sd and append them
        global_average_value_sd[i] = np.std(global_value_iters)
        local_average_value_sd[i] = np.std(local_value_iters, axis = 0)
        average_ratios_sd[i] = np.std(ratios_iters, axis = 0)
        
        # Store
        if storeCSV:
            DeleteTempResults("global_average_value.csv")
            NumpyToCsv(global_average_value, "global_average_value")
            DeleteTempResults("local_average_value.csv")
            NumpyToCsv(local_average_value, "local_average_value")
            DeleteTempResults("average_ratios.csv")
            NumpyToCsv(average_ratios, "average_ratios")
            DeleteTempResults("global_average_value.csv")
            NumpyToCsv(global_average_value_sd, "global_average_value_sd")
            DeleteTempResults("local_average_value_sd.csv")
            NumpyToCsv(local_average_value_sd, "local_average_value_sd")
            DeleteTempResults("average_ratios_sd.csv")
            NumpyToCsv(average_ratios_sd, "average_ratios_sd")
   
    return global_average_value, local_average_value, average_ratios, global_average_value_sd, local_average_value_sd, average_ratios_sd

nodes = np.linspace(5, 70, num=65)
# Remove decimal part
nodes = np.floor(nodes)
# Convert to integer
nodes = nodes.astype(int)
iters = 50
min_weight = -1
max_weight = 1
heuristics = ["GBF", "RPF", "WF"]
k = 2
initial_partition = "0"
dropout = 0

global_average_value, local_average_value, average_ratios, global_average_value_sd, local_average_value_sd, average_ratios_sd = SolveGlobalAndLocalMaxCut(nodes, iters, min_weight, max_weight, heuristics, k, initial_partition, dropout, storeCSV=True)